"""
åˆ†å¸ƒå¼ GAN è®­ç»ƒ - ä½¿ç”¨ Mini-Ray

æ”¯æŒå¤šç§åˆ†å¸ƒå¼ç­–ç•¥ï¼š
1. æ•°æ®å¹¶è¡Œï¼ˆå¤šä¸ª Worker è®­ç»ƒï¼Œå‘¨æœŸæ€§å‚æ•°åŒæ­¥ï¼‰
2. åˆ†å¸ƒå¼æ•°æ®åŠ è½½
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset

import numpy as np
import time
import os
import sys

# å¯¼å…¥ miniray python è·¯å¾„
_current_dir = os.path.dirname(os.path.abspath(__file__))
_python_path = os.path.join(_current_dir, '..', 'python')
if _python_path not in sys.path:
    sys.path.insert(0, _python_path)

import miniray
from ml.gan_cifar10 import Generator, Discriminator


# ============================================================
# åˆ†å¸ƒå¼ Workerï¼šè®­ç»ƒ CIFAR-10 GAN
# ============================================================

@miniray.remote
class DistributedGANWorker:
    """æ¯ä¸ª Worker åœ¨è‡ªå·±çš„æ•°æ®åˆ†ç‰‡ä¸Šè®­ç»ƒ GAN"""

    def __init__(self, worker_id, latent_dim=100, lr=0.0002, device=None):
        self.worker_id = worker_id
        self.latent_dim = latent_dim
        self.lr = lr
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        print(f"[Worker {worker_id}] åˆå§‹åŒ– - è®¾å¤‡: {self.device}")

        self.generator = Generator(latent_dim).to(self.device)
        self.discriminator = Discriminator().to(self.device)

        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))
        self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))

        self.criterion = nn.BCELoss()
        self.dataloader = None

    def load_data_shard(self, shard_id, num_shards, batch_size=128):
        """åŠ è½½æ•°æ®åˆ†ç‰‡"""

        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (0.5,))
        ])

        dataset = torchvision.datasets.CIFAR10(
            root="./data",
            train=True,
            download=False,
            transform=transform
        )

        total_size = len(dataset)
        per_shard = total_size // num_shards

        start = shard_id * per_shard
        end = total_size if shard_id == num_shards - 1 else start + per_shard

        indices = list(range(start, end))
        shard = Subset(dataset, indices)

        self.dataloader = DataLoader(
            shard,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0
        )

        print(f"[Worker {self.worker_id}] åŠ è½½æ•°æ® {len(shard)} å¼ å›¾åƒ")

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ª epoch"""

        g_total = 0
        d_total = 0
        batches = 0

        for real_images, _ in self.dataloader:
            real_images = real_images.to(self.device)
            bsz = real_images.size(0)

            real_labels = torch.ones(bsz, 1).to(self.device)
            fake_labels = torch.zeros(bsz, 1).to(self.device)

            # ========== Train D ==========
            self.optimizer_D.zero_grad()

            out_real = self.discriminator(real_images)
            loss_real = self.criterion(out_real, real_labels)

            z = torch.randn(bsz, self.latent_dim).to(self.device)
            fake = self.generator(z)
            out_fake = self.discriminator(fake.detach())
            loss_fake = self.criterion(out_fake, fake_labels)

            d_loss = loss_real + loss_fake
            d_loss.backward()
            self.optimizer_D.step()

            # ========== Train G ==========
            self.optimizer_G.zero_grad()

            z = torch.randn(bsz, self.latent_dim).to(self.device)
            fake = self.generator(z)
            out_fake = self.discriminator(fake)
            g_loss = self.criterion(out_fake, real_labels)

            g_loss.backward()
            self.optimizer_G.step()

            g_total += g_loss.item()
            d_total += d_loss.item()
            batches += 1

        return {
            "worker_id": self.worker_id,
            "epoch": epoch,
            "g_loss": g_total / batches,
            "d_loss": d_total / batches
        }

    # ============================
    # å‚æ•°åŒæ­¥æ¥å£ï¼ˆçº¿æ€§åŒ–å‚æ•°åˆ—è¡¨ï¼‰
    # ============================

    def get_weights(self):
        """è¿”å› generator å’Œ discriminator çš„å‚æ•°ï¼ˆçº¿æ€§åŒ–ï¼‰"""
        weights = []

        for param in self.generator.state_dict().values():
            weights.append(param.detach().cpu())

        for param in self.discriminator.state_dict().values():
            weights.append(param.detach().cpu())

        return weights

    def set_weights(self, weights):
        """ä»çº¿æ€§åŒ–å‚æ•°åˆ—è¡¨æ¢å¤æ¨¡å‹å‚æ•°"""

        idx = 0

        gen_sd = self.generator.state_dict()
        for key in gen_sd.keys():
            gen_sd[key] = weights[idx].to(self.device)
            idx += 1
        self.generator.load_state_dict(gen_sd)

        disc_sd = self.discriminator.state_dict()
        for key in disc_sd.keys():
            disc_sd[key] = weights[idx].to(self.device)
            idx += 1
        self.discriminator.load_state_dict(disc_sd)

    def save_models(self, save_dir):
        os.makedirs(save_dir, exist_ok=True)
        torch.save(self.generator.state_dict(), f"{save_dir}/generator_{self.worker_id}.pth")
        torch.save(self.discriminator.state_dict(), f"{save_dir}/discriminator_{self.worker_id}.pth")
        return f"[Worker {self.worker_id}] æ¨¡å‹å·²ä¿å­˜"


# ============================================================
# Trainerï¼šåˆ†å¸ƒå¼ GAN è°ƒåº¦å™¨
# ============================================================

class DistributedGANTrainer:

    def __init__(self, num_workers=4, latent_dim=100, lr=0.0002):
        self.num_workers = num_workers
        self.latent_dim = latent_dim
        self.lr = lr

        print("\n[DistributedGANTrainer] åˆå§‹åŒ–")
        print(f" Workers: {num_workers}")
        print(f" Latent Dim: {latent_dim}")
        print(f" Learning Rate: {lr}")

    def train(self, epochs=50, batch_size=128, sync_interval=5):
        print("========== å¯åŠ¨ Mini-Ray ==========")
        miniray.init(num_workers=self.num_workers)

        print("========== åˆ›å»º Workers ==========")
        workers = [
            DistributedGANWorker.remote(i, self.latent_dim, self.lr)
            for i in range(self.num_workers)
        ]

        print("========== åŠ è½½æ•°æ® ==========")
        miniray.get([
            w.load_data_shard.remote(i, self.num_workers, batch_size)
            for i, w in enumerate(workers)
        ])

        history = []

        for epoch in range(epochs):
            print(f"\n===== Epoch {epoch+1}/{epochs} =====")

            # --------- å¹¶è¡Œè®­ç»ƒ ---------
            results = miniray.get([
                w.train_epoch.remote(epoch)
                for w in workers
            ])

            g_loss = np.mean([r["g_loss"] for r in results])
            d_loss = np.mean([r["d_loss"] for r in results])
            history.append((g_loss, d_loss))

            print(f"G_loss = {g_loss:.4f}, D_loss = {d_loss:.4f}")

            # --------- å‚æ•°åŒæ­¥ ---------
            if (epoch + 1) % sync_interval == 0:
                print("ğŸ”„ åŒæ­¥å‚æ•°...")

                weight_lists = miniray.get([w.get_weights.remote() for w in workers])

                num_params = len(weight_lists[0])
                avg_weights = []

                for p in range(num_params):
                    tensors = [weight_lists[w][p] for w in range(self.num_workers)]
                    t0 = tensors[0]

                    if torch.is_floating_point(t0):
                        avg = torch.stack(tensors).mean(0)
                    else:
                        avg = t0  # int/bool ä¸èƒ½å¹³å‡

                    avg_weights.append(avg)

                miniray.get([w.set_weights.remote(avg_weights) for w in workers])
                print("âœ… æƒé‡åŒæ­¥å®Œæˆ")

        return history, workers


# ============================================================
# åˆ†å¸ƒå¼å›¾ç‰‡ç”Ÿæˆ Worker
# ============================================================

@miniray.remote
class DistributedImageGenerator:

    def __init__(self, worker_id, latent_dim=100, device=None):
        self.worker_id = worker_id
        self.latent_dim = latent_dim
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        print(f"[Generator Worker {worker_id}] åˆå§‹åŒ– - è®¾å¤‡: {self.device}")
        self.generator = None

    def load_model(self, model_path):
        self.generator = Generator(self.latent_dim).to(self.device)
        self.generator.load_state_dict(torch.load(model_path, map_location=self.device))
        self.generator.eval()
        return f"Worker {self.worker_id} æ¨¡å‹åŠ è½½å®Œæˆ"

    def generate_batch(self, num_images, seed_offset=0):
        if self.generator is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ load_model()")

        torch.manual_seed(seed_offset + self.worker_id)
        np.random.seed(seed_offset + self.worker_id)

        with torch.no_grad():
            z = torch.randn(num_images, self.latent_dim).to(self.device)
            imgs = self.generator(z)

            imgs = imgs.cpu().numpy()
            imgs = np.transpose(imgs, (0, 2, 3, 1))  # CHW â†’ HWC
            imgs = (imgs + 1) / 2.0
            imgs = np.clip(imgs, 0, 1)

        return imgs


# ============================================================
# åˆ†å¸ƒå¼å›¾ç‰‡ç”Ÿæˆåè°ƒå™¨
# ============================================================

class DistributedImageGeneratorCoordinator:

    def __init__(self, num_workers=4, latent_dim=100):
        self.num_workers = num_workers
        self.latent_dim = latent_dim

    def generate(self, model_path, num_images=100, save_dir="./generated_images", seed=42):

        print("\n========== åˆ†å¸ƒå¼å›¾ç‰‡ç”Ÿæˆ ==========")

        if not os.path.exists(model_path):
            raise FileNotFoundError(model_path)

        miniray.init(num_workers=self.num_workers)

        workers = [
            DistributedImageGenerator.remote(i, self.latent_dim)
            for i in range(self.num_workers)
        ]

        miniray.get([w.load_model.remote(model_path) for w in workers])

        per_worker = num_images // self.num_workers
        remain = num_images % self.num_workers

        refs = []
        for i, w in enumerate(workers):
            n = per_worker + (remain if i == self.num_workers - 1 else 0)
            refs.append(w.generate_batch.remote(n, seed + i * 1000))

        batches = miniray.get(refs)
        all_imgs = np.concatenate(batches, axis=0)

        os.makedirs(save_dir, exist_ok=True)
        from PIL import Image

        for i, img in enumerate(all_imgs):
            Image.fromarray((img * 255).astype(np.uint8)).save(
                f"{save_dir}/generated_{i:04d}.png"
            )

        print(f"å·²ç”Ÿæˆ {len(all_imgs)} å¼ å›¾ç‰‡ï¼Œä¿å­˜äº {save_dir}")

        return all_imgs
