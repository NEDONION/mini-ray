"""
åˆ†å¸ƒå¼ GAN è®­ç»ƒ - ä½¿ç”¨ Mini-Ray

ä½¿ç”¨ Mini-Ray çš„ @remote è£…é¥°å™¨å®ç°åˆ†å¸ƒå¼è®­ç»ƒ
æ”¯æŒå¤šç§åˆ†å¸ƒå¼ç­–ç•¥ï¼š
1. æ•°æ®å¹¶è¡Œ - å¤šä¸ª Worker å¹¶è¡Œè®­ç»ƒï¼Œå®šæœŸåŒæ­¥å‚æ•°
2. åˆ†å¸ƒå¼æ•°æ®åŠ è½½ - å¹¶è¡ŒåŠ è½½å’Œé¢„å¤„ç†æ•°æ®
"""
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
import numpy as np
import time
import os
import sys

# å¯¼å…¥ miniray - æ·»åŠ  python è·¯å¾„
_current_dir = os.path.dirname(os.path.abspath(__file__))
_python_path = os.path.join(_current_dir, '..', 'python')
if _python_path not in sys.path:
    sys.path.insert(0, _python_path)
import miniray

from ml.gan_cifar10 import Generator, Discriminator


# ============================================================
# åˆ†å¸ƒå¼è®­ç»ƒ Worker
# ============================================================

@miniray.remote
class DistributedGANWorker:
    """
    åˆ†å¸ƒå¼ GAN è®­ç»ƒ Worker

    æ¯ä¸ª Worker åœ¨è‡ªå·±çš„æ•°æ®åˆ†ç‰‡ä¸Šè®­ç»ƒ GAN
    """

    def __init__(self, worker_id, latent_dim=100, lr=0.0002, device=None):
        self.worker_id = worker_id
        self.latent_dim = latent_dim
        self.lr = lr
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')

        print(f"[Worker {worker_id}] åˆå§‹åŒ–åˆ†å¸ƒå¼ GAN Worker - è®¾å¤‡: {self.device}")

        # åˆ›å»ºç½‘ç»œ
        self.generator = Generator(latent_dim).to(self.device)
        self.discriminator = Discriminator().to(self.device)

        # ä¼˜åŒ–å™¨
        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=lr, betas=(0.5, 0.999))
        self.optimizer_D = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(0.5, 0.999))

        # æŸå¤±å‡½æ•°
        self.criterion = nn.BCELoss()

    def load_data_shard(self, shard_id, num_shards, batch_size=128):
        """
        åŠ è½½æ•°æ®åˆ†ç‰‡

        Args:
            shard_id: åˆ†ç‰‡ ID
            num_shards: æ€»åˆ†ç‰‡æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])

        # åŠ è½½å®Œæ•´æ•°æ®é›†
        full_dataset = torchvision.datasets.CIFAR10(
            root='./data',
            train=True,
            download=False,
            transform=transform
        )

        # è®¡ç®—æ­¤ Worker è´Ÿè´£çš„æ•°æ®èŒƒå›´
        total_size = len(full_dataset)
        shard_size = total_size // num_shards
        start_idx = shard_id * shard_size
        end_idx = start_idx + shard_size if shard_id < num_shards - 1 else total_size

        # åˆ›å»ºæ•°æ®åˆ†ç‰‡
        indices = list(range(start_idx, end_idx))
        shard_dataset = Subset(full_dataset, indices)

        self.dataloader = DataLoader(
            shard_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=0  # é¿å…åœ¨ Worker ä¸­å†åˆ›å»ºå­è¿›ç¨‹
        )

        print(f"[Worker {self.worker_id}] æ•°æ®åˆ†ç‰‡ {shard_id}/{num_shards}: {len(shard_dataset)} å¼ å›¾ç‰‡")

    def train_epoch(self, epoch):
        """
        è®­ç»ƒä¸€ä¸ª epoch

        Args:
            epoch: å½“å‰ epoch

        Returns:
            è®­ç»ƒæŒ‡æ ‡å­—å…¸
        """
        epoch_g_loss = 0.0
        epoch_d_loss = 0.0
        num_batches = 0

        for i, (real_images, _) in enumerate(self.dataloader):
            batch_size = real_images.size(0)
            real_images = real_images.to(self.device)

            # æ ‡ç­¾
            real_labels = torch.ones(batch_size, 1).to(self.device)
            fake_labels = torch.zeros(batch_size, 1).to(self.device)

            # ========================================
            # è®­ç»ƒ Discriminator
            # ========================================
            self.optimizer_D.zero_grad()

            # çœŸå®å›¾ç‰‡
            real_output = self.discriminator(real_images)
            d_loss_real = self.criterion(real_output, real_labels)

            # å‡å›¾ç‰‡
            z = torch.randn(batch_size, self.latent_dim).to(self.device)
            fake_images = self.generator(z)
            fake_output = self.discriminator(fake_images.detach())
            d_loss_fake = self.criterion(fake_output, fake_labels)

            d_loss = d_loss_real + d_loss_fake
            d_loss.backward()
            self.optimizer_D.step()

            # ========================================
            # è®­ç»ƒ Generator
            # ========================================
            self.optimizer_G.zero_grad()

            z = torch.randn(batch_size, self.latent_dim).to(self.device)
            fake_images = self.generator(z)
            fake_output = self.discriminator(fake_images)

            g_loss = self.criterion(fake_output, real_labels)
            g_loss.backward()
            self.optimizer_G.step()

            epoch_g_loss += g_loss.item()
            epoch_d_loss += d_loss.item()
            num_batches += 1

        # è¿”å›å¹³å‡æŸå¤±
        return {
            'worker_id': self.worker_id,
            'epoch': epoch,
            'g_loss': epoch_g_loss / num_batches,
            'd_loss': epoch_d_loss / num_batches,
            'num_batches': num_batches
        }

    def get_model_state(self):
        """è·å–æ¨¡å‹å‚æ•°"""
        return {
            'generator': self.generator.state_dict(),
            'discriminator': self.discriminator.state_dict()
        }

    def set_model_state(self, state_dict):
        """è®¾ç½®æ¨¡å‹å‚æ•°"""
        self.generator.load_state_dict(state_dict['generator'])
        self.discriminator.load_state_dict(state_dict['discriminator'])

    def save_models(self, save_dir):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(save_dir, exist_ok=True)
        torch.save(self.generator.state_dict(), f'{save_dir}/generator_worker{self.worker_id}.pth')
        torch.save(self.discriminator.state_dict(), f'{save_dir}/discriminator_worker{self.worker_id}.pth')
        return f"Worker {self.worker_id} æ¨¡å‹å·²ä¿å­˜"


# ============================================================
# åˆ†å¸ƒå¼è®­ç»ƒåè°ƒå™¨
# ============================================================

class DistributedGANTrainer:
    """
    åˆ†å¸ƒå¼ GAN è®­ç»ƒåè°ƒå™¨

    ä½¿ç”¨æ•°æ®å¹¶è¡Œç­–ç•¥ï¼š
    1. å¯åŠ¨å¤šä¸ª Workerï¼Œæ¯ä¸ªå¤„ç†ä¸€éƒ¨åˆ†æ•°æ®
    2. å¹¶è¡Œè®­ç»ƒ
    3. å®šæœŸåŒæ­¥å‚æ•°ï¼ˆå¹³å‡èšåˆï¼‰

    ä½¿ç”¨æ–¹æ³•:
        trainer = DistributedGANTrainer(num_workers=4)
        trainer.train(epochs=50, batch_size=128, job_id='dist-gan-001')
    """

    def __init__(self, num_workers=4, latent_dim=100, lr=0.0002):
        """
        åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒå™¨

        Args:
            num_workers: Worker æ•°é‡
            latent_dim: éšå˜é‡ç»´åº¦
            lr: å­¦ä¹ ç‡
        """
        self.num_workers = num_workers
        self.latent_dim = latent_dim
        self.lr = lr

        print(f"\n[DistributedGANTrainer] åˆå§‹åŒ–")
        print(f"  Workers: {num_workers}")
        print(f"  Latent Dim: {latent_dim}")
        print(f"  Learning Rate: {lr}")

    def train(self, epochs=50, batch_size=128, sync_interval=5, job_id=None, collector=None):
        """
        å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒ

        Args:
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            sync_interval: å‚æ•°åŒæ­¥é—´éš”ï¼ˆæ¯ N ä¸ª epochï¼‰
            job_id: ä»»åŠ¡ ID
            collector: Dashboard æ”¶é›†å™¨

        Returns:
            è®­ç»ƒå†å²
        """
        print(f"\n{'='*70}")
        print(f"  å¼€å§‹åˆ†å¸ƒå¼ GAN è®­ç»ƒ")
        print(f"{'='*70}")
        print(f"  Epochs: {epochs}")
        print(f"  Batch Size: {batch_size}")
        print(f"  Sync Interval: {sync_interval} epochs")
        print(f"  Workers: {self.num_workers}")
        print()

        # åˆå§‹åŒ– Mini-Ray
        if not hasattr(miniray, '_initialized') or not miniray._initialized:
            miniray.init(num_workers=self.num_workers)
            print(f"âœ… Mini-Ray å·²åˆå§‹åŒ– ({self.num_workers} workers)")

        # Dashboard åˆå§‹åŒ–
        if collector and job_id:
            collector.record_training_job(
                job_id=job_id,
                name=f"Distributed GAN Training ({self.num_workers} Workers)",
                status='Running',
                progress=0.0,
                config={
                    'model': 'Distributed GAN',
                    'dataset': 'CIFAR-10',
                    'num_workers': self.num_workers,
                    'epochs': epochs,
                    'batch_size': batch_size,
                    'sync_interval': sync_interval
                },
                metrics={}
            )
            collector.add_training_log(job_id, f"[INFO] åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨ - {self.num_workers} Workers")

        # åˆ›å»ºåˆ†å¸ƒå¼ Workers
        print(f"ğŸš€ åˆ›å»º {self.num_workers} ä¸ªè®­ç»ƒ Workers...")
        workers = []
        for i in range(self.num_workers):
            worker = DistributedGANWorker.remote(
                worker_id=i,
                latent_dim=self.latent_dim,
                lr=self.lr
                # device å‚æ•°å·²çœç•¥ï¼Œä¼šè‡ªåŠ¨æ£€æµ‹ï¼šä¼˜å…ˆ GPUï¼Œæ—  GPU åˆ™ç”¨ CPU
            )
            workers.append(worker)

        # åŠ è½½æ•°æ®åˆ†ç‰‡
        print(f"ğŸ“¦ åˆ†å‘æ•°æ®åˆ°å„ä¸ª Worker...")
        load_refs = []
        for i, worker in enumerate(workers):
            ref = worker.load_data_shard.remote(
                shard_id=i,
                num_shards=self.num_workers,
                batch_size=batch_size
            )
            load_refs.append(ref)

        # ç­‰å¾…æ•°æ®åŠ è½½å®Œæˆï¼ˆæ•°æ®åŠ è½½å¯èƒ½éœ€è¦æ—¶é—´ä¸‹è½½ CIFAR-10ï¼‰
        miniray.get(load_refs, timeout_s=300.0)  # 5 åˆ†é’Ÿè¶…æ—¶
        print("âœ… æ‰€æœ‰ Worker æ•°æ®åŠ è½½å®Œæˆ\n")

        if collector and job_id:
            collector.add_training_log(job_id, f"[INFO] æ•°æ®åˆ†ç‰‡å®Œæˆï¼Œæ¯ä¸ª Worker å¤„ç† 1/{self.num_workers} çš„æ•°æ®")

        history = {
            'epochs': [],
            'avg_g_loss': [],
            'avg_d_loss': []
        }

        # è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            epoch_start = time.time()

            # å¹¶è¡Œè®­ç»ƒä¸€ä¸ª epoch
            print(f"[Epoch {epoch+1}/{epochs}] æ‰€æœ‰ Workers å¹¶è¡Œè®­ç»ƒ...")
            train_refs = []
            for worker in workers:
                ref = worker.train_epoch.remote(epoch)
                train_refs.append(ref)

            # æ”¶é›†ç»“æœï¼ˆGAN è®­ç»ƒä¸€ä¸ª epoch å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰
            results = miniray.get(train_refs, timeout_s=600.0)  # 10 åˆ†é’Ÿè¶…æ—¶

            # èšåˆç»“æœ
            avg_g_loss = np.mean([r['g_loss'] for r in results])
            avg_d_loss = np.mean([r['d_loss'] for r in results])
            epoch_time = time.time() - epoch_start
            progress = ((epoch + 1) / epochs) * 100

            history['epochs'].append(epoch + 1)
            history['avg_g_loss'].append(avg_g_loss)
            history['avg_d_loss'].append(avg_d_loss)

            print(f"[Epoch {epoch+1}/{epochs}] å®Œæˆ:")
            print(f"  Avg G_loss: {avg_g_loss:.4f} (across {self.num_workers} workers)")
            print(f"  Avg D_loss: {avg_d_loss:.4f}")
            print(f"  Time: {epoch_time:.2f}s")
            print(f"  Progress: {progress:.1f}%\n")

            # æ›´æ–° Dashboard
            if collector and job_id:
                collector.record_training_job(
                    job_id=job_id,
                    name=f"Distributed GAN Training ({self.num_workers} Workers)",
                    status='Running',
                    progress=progress,
                    config={
                        'model': 'Distributed GAN',
                        'num_workers': self.num_workers,
                        'current_epoch': epoch + 1,
                        'total_epochs': epochs
                    },
                    metrics={
                        'g_loss': float(avg_g_loss),
                        'd_loss': float(avg_d_loss),
                        'epoch': epoch + 1
                    }
                )
                collector.add_training_log(
                    job_id,
                    f"[INFO] Epoch {epoch+1}/{epochs} | Avg G_loss: {avg_g_loss:.4f} | Avg D_loss: {avg_d_loss:.4f} | {epoch_time:.1f}s"
                )

            # å‚æ•°åŒæ­¥ï¼ˆæ¯ N ä¸ª epochï¼‰
            if (epoch + 1) % sync_interval == 0 and epoch < epochs - 1:
                print(f"ğŸ”„ åŒæ­¥æ¨¡å‹å‚æ•°...")

                # è·å–æ‰€æœ‰ Worker çš„å‚æ•°
                state_refs = [worker.get_model_state.remote() for worker in workers]
                states = miniray.get(state_refs, timeout_s=60.0)

                # å¹³å‡èšåˆå‚æ•°
                avg_state = self._average_model_states(states)

                # å¹¿æ’­åˆ°æ‰€æœ‰ Worker
                sync_refs = [worker.set_model_state.remote(avg_state) for worker in workers]
                miniray.get(sync_refs, timeout_s=60.0)

                print(f"âœ… å‚æ•°åŒæ­¥å®Œæˆ\n")

                if collector and job_id:
                    collector.add_training_log(job_id, f"[INFO] å‚æ•°åŒæ­¥å®Œæˆï¼ˆEpoch {epoch+1}ï¼‰")

        # è®­ç»ƒå®Œæˆ
        print(f"\n{'='*70}")
        print(f"  âœ… åˆ†å¸ƒå¼è®­ç»ƒå®Œæˆï¼")
        print(f"{'='*70}")

        if collector and job_id:
            collector.record_training_job(
                job_id=job_id,
                name=f"Distributed GAN Training ({self.num_workers} Workers)",
                status='Completed',
                progress=100.0,
                config={'model': 'Distributed GAN', 'num_workers': self.num_workers},
                metrics={
                    'final_g_loss': float(history['avg_g_loss'][-1]),
                    'final_d_loss': float(history['avg_d_loss'][-1])
                }
            )
            collector.add_training_log(job_id, f"[SUCCESS] è®­ç»ƒå®Œæˆï¼")

        # ä¿å­˜æ¨¡å‹
        print("\nğŸ’¾ ä¿å­˜æ‰€æœ‰ Worker çš„æ¨¡å‹...")
        save_refs = [worker.save_models.remote(f'./models/distributed_gan/worker_{i}')
                     for i, worker in enumerate(workers)]
        messages = miniray.get(save_refs, timeout_s=60.0)
        for msg in messages:
            print(f"  {msg}")

        return history, workers

    def _average_model_states(self, states):
        """
        èšåˆæ¥è‡ªå¤šä¸ª Worker çš„æ¨¡å‹å‚æ•°ï¼ˆå‚æ•°å¹³å‡ï¼‰ã€‚

        å¤„ç†è§„åˆ™ï¼š
        - æµ®ç‚¹å¼ é‡ï¼ˆæƒé‡/åç½®/BN running statsï¼‰ï¼š
            ä½¿ç”¨ stack + mean åšå‚æ•°å¹³å‡ã€‚
        - æ•´å‹/å¸ƒå°”å¼ é‡ï¼ˆå¦‚ BatchNorm.num_batches_trackedï¼‰ï¼š
            æ— æ³•å¹³å‡ï¼Œä¹Ÿæ²¡æœ‰å¹³å‡æ„ä¹‰ï¼Œç›´æ¥å–ç¬¬ 0 ä¸ª Worker çš„å€¼ã€‚
        - é Tensor ç±»å‹ï¼š
            ç›´æ¥å–ç¬¬ 0 ä¸ª Worker çš„å€¼ã€‚

        å‚æ•°:
            states: List[Dict]
                æ¯ä¸ª Worker è¿”å›çš„ state_dict åˆ—è¡¨ã€‚

        è¿”å›:
            avg_state: Dict
                èšåˆåçš„æ¨¡å‹å‚æ•°å­—å…¸ã€‚
        """
        avg_state = {}

        for model_name in ['generator', 'discriminator']:
            avg_state[model_name] = {}
            param_names = states[0][model_name].keys()

            for param_name in param_names:
                params = [s[model_name][param_name] for s in states]
                first = params[0]

                # é tensorï¼ˆæ¯”å¦‚ Noneã€æ ‡é‡ï¼‰ç›´æ¥æ‹¿ç¬¬ä¸€ä¸ª
                if not isinstance(first, torch.Tensor):
                    avg_state[model_name][param_name] = first
                    continue

                # æµ®ç‚¹ / å¤æ•°ï¼šåšå¹³å‡
                if torch.is_floating_point(first) or torch.is_complex(first):
                    stacked = torch.stack(params, dim=0)
                    avg_param = stacked.mean(dim=0)
                    avg_state[model_name][param_name] = avg_param
                else:
                    # æ•´å‹ / Boolï¼šç›´æ¥ç”¨ç¬¬ä¸€ä¸ª worker çš„å€¼å³å¯
                    # å…¸å‹å¦‚ BatchNorm.num_batches_tracked
                    avg_state[model_name][param_name] = first

        return avg_state


# ============================================================
# åˆ†å¸ƒå¼å›¾ç‰‡ç”Ÿæˆ
# ============================================================

@miniray.remote
class DistributedImageGenerator:
    """
    åˆ†å¸ƒå¼å›¾ç‰‡ç”Ÿæˆ Worker

    æ¯ä¸ª Worker å¹¶è¡Œç”Ÿæˆä¸€éƒ¨åˆ†å›¾ç‰‡
    """

    def __init__(self, worker_id, latent_dim=100, device=None):
        self.worker_id = worker_id
        self.latent_dim = latent_dim
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.generator = None
        print(f"[Generator Worker {worker_id}] åˆå§‹åŒ– - è®¾å¤‡: {self.device}")

    def load_model(self, model_path):
        """åŠ è½½ç”Ÿæˆå™¨æ¨¡å‹"""
        self.generator = Generator(self.latent_dim).to(self.device)
        self.generator.load_state_dict(torch.load(model_path, map_location=self.device))
        self.generator.eval()
        return f"Worker {self.worker_id} æ¨¡å‹åŠ è½½å®Œæˆ"

    def generate_batch(self, num_images, seed_offset=0):
        """
        ç”Ÿæˆä¸€æ‰¹å›¾ç‰‡

        Args:
            num_images: ç”Ÿæˆæ•°é‡
            seed_offset: éšæœºç§å­åç§»ï¼ˆç”¨äºç¡®ä¿ä¸åŒ Worker ç”Ÿæˆä¸åŒå›¾ç‰‡ï¼‰

        Returns:
            ç”Ÿæˆçš„å›¾ç‰‡æ•°ç»„ (N, H, W, C)
        """
        if self.generator is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")

        # è®¾ç½®éšæœºç§å­
        if seed_offset is not None:
            torch.manual_seed(seed_offset + self.worker_id)
            np.random.seed(seed_offset + self.worker_id)

        with torch.no_grad():
            # ç”Ÿæˆéšæœºå™ªå£°
            z = torch.randn(num_images, self.latent_dim).to(self.device)

            # ç”Ÿæˆå›¾ç‰‡
            fake_images = self.generator(z)

            # è½¬æ¢åˆ° numpy æ ¼å¼: (N, C, H, W) -> (N, H, W, C)
            images = fake_images.cpu().numpy()
            images = np.transpose(images, (0, 2, 3, 1))

            # åå½’ä¸€åŒ–: [-1, 1] -> [0, 1]
            images = (images + 1) / 2.0
            images = np.clip(images, 0, 1)

        return images


class DistributedImageGeneratorCoordinator:
    """
    åˆ†å¸ƒå¼å›¾ç‰‡ç”Ÿæˆåè°ƒå™¨

    ä½¿ç”¨å¤šä¸ª Worker å¹¶è¡Œç”Ÿæˆå¤§é‡å›¾ç‰‡

    ä½¿ç”¨æ–¹æ³•:
        coordinator = DistributedImageGeneratorCoordinator(num_workers=4)
        images = coordinator.generate(
            model_path='./models/gan/generator.pth',
            num_images=100,
            save_dir='./output'
        )
    """

    def __init__(self, num_workers=4, latent_dim=100):
        """
        åˆå§‹åŒ–åˆ†å¸ƒå¼ç”Ÿæˆåè°ƒå™¨

        Args:
            num_workers: Worker æ•°é‡
            latent_dim: éšå˜é‡ç»´åº¦
        """
        self.num_workers = num_workers
        self.latent_dim = latent_dim

        print(f"\n[DistributedImageGenerator] åˆå§‹åŒ–")
        print(f"  Workers: {num_workers}")
        print(f"  Latent Dim: {latent_dim}")

    def generate(self, model_path, num_images=100, save_dir='./generated_images', seed=42):
        """
        å¹¶è¡Œç”Ÿæˆå›¾ç‰‡

        Args:
            model_path: ç”Ÿæˆå™¨æ¨¡å‹è·¯å¾„
            num_images: æ€»å…±ç”Ÿæˆçš„å›¾ç‰‡æ•°é‡
            save_dir: ä¿å­˜ç›®å½•
            seed: éšæœºç§å­

        Returns:
            æ‰€æœ‰ç”Ÿæˆçš„å›¾ç‰‡æ•°ç»„
        """
        print(f"\n{'='*70}")
        print(f"  åˆ†å¸ƒå¼å›¾ç‰‡ç”Ÿæˆ")
        print(f"{'='*70}")
        print(f"  Model: {model_path}")
        print(f"  Total Images: {num_images}")
        print(f"  Workers: {self.num_workers}")
        print(f"  Images per Worker: ~{num_images // self.num_workers}")
        print()

        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

        # åˆå§‹åŒ– Mini-Ray
        if not hasattr(miniray, '_initialized') or not miniray._initialized:
            miniray.init(num_workers=self.num_workers)
            print(f"âœ… Mini-Ray å·²åˆå§‹åŒ– ({self.num_workers} workers)")

        # åˆ›å»ºåˆ†å¸ƒå¼ç”Ÿæˆ Workers
        print(f"ğŸš€ åˆ›å»º {self.num_workers} ä¸ªç”Ÿæˆ Workers...")
        workers = []
        for i in range(self.num_workers):
            worker = DistributedImageGenerator.remote(
                worker_id=i,
                latent_dim=self.latent_dim
                # device å‚æ•°å·²çœç•¥ï¼Œä¼šè‡ªåŠ¨æ£€æµ‹ï¼šä¼˜å…ˆ GPUï¼Œæ—  GPU åˆ™ç”¨ CPU
            )
            workers.append(worker)

        # åŠ è½½æ¨¡å‹åˆ°æ‰€æœ‰ Workers
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹åˆ°æ‰€æœ‰ Workers...")
        load_refs = [worker.load_model.remote(model_path) for worker in workers]
        messages = miniray.get(load_refs, timeout_s=60.0)
        for msg in messages:
            print(f"  {msg}")

        # è®¡ç®—æ¯ä¸ª Worker ç”Ÿæˆçš„å›¾ç‰‡æ•°é‡
        images_per_worker = num_images // self.num_workers
        remaining = num_images % self.num_workers

        # å¹¶è¡Œç”Ÿæˆå›¾ç‰‡
        print(f"\nğŸ¨ å¼€å§‹å¹¶è¡Œç”Ÿæˆ {num_images} å¼ å›¾ç‰‡...")
        gen_refs = []
        for i, worker in enumerate(workers):
            # æœ€åä¸€ä¸ª Worker å¤„ç†ä½™æ•°
            worker_images = images_per_worker + (remaining if i == self.num_workers - 1 else 0)
            ref = worker.generate_batch.remote(
                num_images=worker_images,
                seed_offset=seed + i * 1000 if seed else None
            )
            gen_refs.append(ref)

        # æ”¶é›†ç»“æœ
        print(f"ğŸ“¥ ç­‰å¾…æ‰€æœ‰ Workers å®Œæˆ...")
        image_batches = miniray.get(gen_refs, timeout_s=300.0)

        # åˆå¹¶æ‰€æœ‰å›¾ç‰‡
        all_images = np.concatenate(image_batches, axis=0)
        print(f"âœ… å…±ç”Ÿæˆ {len(all_images)} å¼ å›¾ç‰‡")

        # ä¿å­˜å›¾ç‰‡
        print(f"\nğŸ’¾ ä¿å­˜å›¾ç‰‡åˆ° {save_dir}...")
        os.makedirs(save_dir, exist_ok=True)

        from PIL import Image
        for i, img in enumerate(all_images):
            # è½¬æ¢ä¸º PIL Image å¹¶ä¿å­˜
            img_pil = Image.fromarray((img * 255).astype(np.uint8))
            img_pil.save(f'{save_dir}/generated_{i:04d}.png')

        print(f"âœ… æ‰€æœ‰å›¾ç‰‡å·²ä¿å­˜åˆ° {save_dir}")
        print(f"\n{'='*70}")
        print(f"  âœ… åˆ†å¸ƒå¼ç”Ÿæˆå®Œæˆï¼")
        print(f"{'='*70}\n")

        return all_images
