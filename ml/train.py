#!/usr/bin/env python3
"""
GAN è®­ç»ƒæ¨¡å— - æ”¯æŒå•æœºå’Œåˆ†å¸ƒå¼è®­ç»ƒ
"""
import sys
import os
# æ·»åŠ  python è·¯å¾„ä»¥å¯¼å…¥ miniray
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'python'))

import time
import argparse
from ml.gan_cifar10 import GANTrainer
from ml.distributed_gan import DistributedGANTrainer


def train_single(epochs=10, batch_size=128, latent_dim=100, lr=0.0002, save_dir='./models/gan'):
    """
    å•æœºè®­ç»ƒ

    Args:
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        latent_dim: éšå˜é‡ç»´åº¦
        lr: å­¦ä¹ ç‡
        save_dir: æ¨¡å‹ä¿å­˜ç›®å½•

    Returns:
        è®­ç»ƒå†å²
    """
    from miniray.dashboard import get_collector

    ensure_cifar10_downloaded(root="./data")

    print("\n" + "="*70)
    print("  å•æœº GAN è®­ç»ƒ")
    print("="*70)
    print(f"\né…ç½®:")
    print(f"  Epochs: {epochs}")
    print(f"  Batch Size: {batch_size}")
    print(f"  Latent Dim: {latent_dim}")
    print(f"  Learning Rate: {lr}")
    print()

    collector = get_collector()
    job_id = f"gan-single-{int(time.time())}"

    trainer = GANTrainer(latent_dim=latent_dim, lr=lr)

    history = trainer.train(
        epochs=epochs,
        batch_size=batch_size,
        job_id=job_id,
        collector=collector
    )

    trainer.save_models(save_dir)

    print(f"\nâœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {save_dir}")
    return history


def train_distributed(epochs=10, batch_size=128, num_workers=4, sync_interval=5,
                      latent_dim=100, lr=0.0002):
    """
    åˆ†å¸ƒå¼è®­ç»ƒ

    Args:
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        num_workers: Worker æ•°é‡
        sync_interval: å‚æ•°åŒæ­¥é—´éš”
        latent_dim: éšå˜é‡ç»´åº¦
        lr: å­¦ä¹ ç‡

    Returns:
        è®­ç»ƒå†å², workers
    """
    from miniray.dashboard import get_collector

    ensure_cifar10_downloaded(root="./data")

    print("\n" + "="*70)
    print("  åˆ†å¸ƒå¼ GAN è®­ç»ƒ")
    print("="*70)
    print(f"\né…ç½®:")
    print(f"  Epochs: {epochs}")
    print(f"  Batch Size: {batch_size}")
    print(f"  Workers: {num_workers}")
    print(f"  Sync Interval: {sync_interval} epochs")
    print(f"  Latent Dim: {latent_dim}")
    print(f"  Learning Rate: {lr}")
    print()

    collector = get_collector()
    job_id = f"gan-dist-{int(time.time())}"

    trainer = DistributedGANTrainer(
        num_workers=num_workers,
        latent_dim=latent_dim,
        lr=lr
    )

    history, workers = trainer.train(
        epochs=epochs,
        batch_size=batch_size,
        sync_interval=sync_interval,
        job_id=job_id,
        collector=collector
    )

    print(f"\nâœ… æ‰€æœ‰ Worker æ¨¡å‹å·²ä¿å­˜åˆ°: ./models/distributed_gan/")
    return history, workers

def ensure_cifar10_downloaded(root="./data"):
    from torchvision.datasets import CIFAR10
    from torchvision import transforms

    if not os.path.exists(os.path.join(root, "cifar-10-batches-py")):
        print("ğŸ“¥ CIFAR-10 æ•°æ®é›†ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä¸‹è½½...")
        CIFAR10(root=root, train=True, download=True, transform=transforms.ToTensor())
        print("âœ… CIFAR-10 ä¸‹è½½å®Œæˆ")
    else:
        print("âœ” CIFAR-10 å·²å­˜åœ¨ï¼Œæ— éœ€ä¸‹è½½")



def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description='GAN Training for CIFAR-10')
    parser.add_argument('--mode', type=str, default='single', choices=['single', 'distributed'],
                        help='è®­ç»ƒæ¨¡å¼ï¼šsingleï¼ˆå•æœºï¼‰æˆ– distributedï¼ˆåˆ†å¸ƒå¼ï¼‰')
    parser.add_argument('--epochs', type=int, default=10, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=128, help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--workers', type=int, default=4, help='Worker æ•°é‡ï¼ˆåˆ†å¸ƒå¼æ¨¡å¼ï¼‰')
    parser.add_argument('--sync-interval', type=int, default=5, help='å‚æ•°åŒæ­¥é—´éš”ï¼ˆåˆ†å¸ƒå¼æ¨¡å¼ï¼‰')
    parser.add_argument('--latent-dim', type=int, default=100, help='éšå˜é‡ç»´åº¦')
    parser.add_argument('--lr', type=float, default=0.0002, help='å­¦ä¹ ç‡')
    parser.add_argument('--save-dir', type=str, default='./models/gan', help='æ¨¡å‹ä¿å­˜ç›®å½•')

    args = parser.parse_args()

    if args.mode == 'single':
        train_single(
            epochs=args.epochs,
            batch_size=args.batch_size,
            latent_dim=args.latent_dim,
            lr=args.lr,
            save_dir=args.save_dir
        )
    else:
        train_distributed(
            epochs=args.epochs,
            batch_size=args.batch_size,
            num_workers=args.workers,
            sync_interval=args.sync_interval,
            latent_dim=args.latent_dim,
            lr=args.lr
        )


if __name__ == '__main__':
    main()
