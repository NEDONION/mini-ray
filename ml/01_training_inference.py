#!/usr/bin/env python3
"""
Mini-Ray ML è®­ç»ƒå’Œæ¨ç† Demo

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Mini-Ray è¿›è¡Œåˆ†å¸ƒå¼æœºå™¨å­¦ä¹ ï¼š
1. æ•°æ®å¹¶è¡Œè®­ç»ƒï¼ˆData Parallel Trainingï¼‰
2. åˆ†å¸ƒå¼æ‰¹é‡æ¨ç†ï¼ˆDistributed Batch Inferenceï¼‰
3. å‚æ•°æœåŠ¡å™¨æ¨¡å¼ï¼ˆParameter Serverï¼‰

ä½¿ç”¨ scikit-learn çš„éšæœºæ£®æ—ä½œä¸ºç¤ºä¾‹æ¨¡å‹
"""
import sys
import os
import time
import numpy as np

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

import miniray


def print_section(title):
    """æ‰“å°åˆ†éš”çº¿å’Œæ ‡é¢˜"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


def generate_synthetic_data(n_samples=10000, n_features=20, n_classes=3, random_state=42):
    """
    ç”Ÿæˆåˆæˆåˆ†ç±»æ•°æ®é›†

    Args:
        n_samples: æ ·æœ¬æ•°é‡
        n_features: ç‰¹å¾æ•°é‡
        n_classes: ç±»åˆ«æ•°é‡
        random_state: éšæœºç§å­

    Returns:
        X: ç‰¹å¾çŸ©é˜µ (n_samples, n_features)
        y: æ ‡ç­¾å‘é‡ (n_samples,)
    """
    np.random.seed(random_state)

    # ç”Ÿæˆç‰¹å¾
    X = np.random.randn(n_samples, n_features)

    # ç”Ÿæˆæ ‡ç­¾ï¼ˆå¤šç±»åˆ†ç±»ï¼‰
    # æ¯ä¸ªç±»æœ‰ä¸€äº›ç‰¹å¾å€¾å‘
    y = np.zeros(n_samples, dtype=int)
    for i in range(n_classes):
        # ç¬¬ i ç±»åœ¨æŸäº›ç‰¹å¾ä¸Šæœ‰æ­£å‘åç§»
        class_indices = np.arange(i * (n_samples // n_classes),
                                   (i + 1) * (n_samples // n_classes))
        y[class_indices] = i
        X[class_indices, i * 5:(i + 1) * 5] += 2.0  # å¢åŠ ç±»é—´åŒºåˆ†åº¦

    # æ‰“ä¹±æ•°æ®
    shuffle_idx = np.random.permutation(n_samples)
    X = X[shuffle_idx]
    y = y[shuffle_idx]

    return X, y


# ============================================================
# Demo 1: æ•°æ®å¹¶è¡Œè®­ç»ƒï¼ˆData Parallel Trainingï¼‰
# ============================================================

def demo_1_data_parallel_training():
    """
    æ¼”ç¤º 1: æ•°æ®å¹¶è¡Œè®­ç»ƒ

    ç­–ç•¥ï¼š
    - å°†æ•°æ®åˆ†ç‰‡ï¼Œæ¯ä¸ª Worker è®­ç»ƒè‡ªå·±çš„æ¨¡å‹å‰¯æœ¬
    - æ¯ä¸ª Worker åœ¨è‡ªå·±çš„æ•°æ®åˆ†ç‰‡ä¸Šè®­ç»ƒ
    - ä¸»è¿›ç¨‹èšåˆæ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœï¼ˆé›†æˆå­¦ä¹ ï¼‰
    """
    print_section("Demo 1: æ•°æ®å¹¶è¡Œè®­ç»ƒï¼ˆData Parallel Trainingï¼‰")

    try:
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score, classification_report
    except ImportError:
        print("âš ï¸  éœ€è¦å®‰è£… scikit-learn: pip install scikit-learn")
        return

    # å®šä¹‰è®­ç»ƒ Worker Actor
    @miniray.remote
    class TrainingWorker:
        def __init__(self, worker_id, n_estimators=50):
            self.worker_id = worker_id
            self.model = RandomForestClassifier(
                n_estimators=n_estimators,
                max_depth=10,
                random_state=worker_id,
                n_jobs=1
            )
            print(f"  [Worker-{worker_id}] åˆå§‹åŒ–å®Œæˆ")

        def train(self, X_train, y_train):
            """è®­ç»ƒæ¨¡å‹"""
            print(f"  [Worker-{self.worker_id}] å¼€å§‹è®­ç»ƒï¼Œæ•°æ®é‡: {len(X_train)}")
            start_time = time.time()

            self.model.fit(X_train, y_train)

            train_time = time.time() - start_time
            train_acc = self.model.score(X_train, y_train)

            print(f"  [Worker-{self.worker_id}] è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {train_time:.2f}sï¼Œè®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")

            return {
                'worker_id': self.worker_id,
                'train_time': train_time,
                'train_accuracy': train_acc,
                'n_samples': len(X_train)
            }

        def predict(self, X_test):
            """é¢„æµ‹"""
            print(f"  [Worker-{self.worker_id}] å¼€å§‹æ¨ç†ï¼Œæ ·æœ¬æ•°: {len(X_test)}")
            predictions = self.model.predict(X_test)
            return predictions

        def get_feature_importance(self):
            """è·å–ç‰¹å¾é‡è¦æ€§"""
            return self.model.feature_importances_

    print("\nğŸ“Œ ç¬¬ 1 æ­¥: ç”Ÿæˆåˆæˆæ•°æ®é›†")
    X, y = generate_synthetic_data(n_samples=10000, n_features=20, n_classes=3)
    print(f"  æ•°æ®é›†å¤§å°: {X.shape}, ç±»åˆ«æ•°: {len(np.unique(y))}")

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    print(f"  è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    print("\nğŸ“Œ ç¬¬ 2 æ­¥: åˆ›å»º 4 ä¸ªè®­ç»ƒ Worker")
    num_workers = 4
    workers = []
    for i in range(num_workers):
        worker = TrainingWorker.remote(worker_id=i, n_estimators=50)
        workers.append(worker)
        print(f"  â†’ åˆ›å»º Worker-{i}")
    time.sleep(0.5)

    print("\nğŸ“Œ ç¬¬ 3 æ­¥: æ•°æ®åˆ†ç‰‡ - å°†è®­ç»ƒæ•°æ®åˆ†é…ç»™å„ä¸ª Worker")
    # å°†è®­ç»ƒæ•°æ®åˆ†æˆ num_workers ä»½
    X_shards = np.array_split(X_train, num_workers)
    y_shards = np.array_split(y_train, num_workers)

    for i in range(num_workers):
        print(f"  Shard-{i}: {X_shards[i].shape[0]} æ ·æœ¬")

    print("\nğŸ“Œ ç¬¬ 4 æ­¥: å¹¶è¡Œè®­ç»ƒæ‰€æœ‰ Worker")
    train_refs = []
    for i, worker in enumerate(workers):
        ref = worker.train.remote(X_shards[i], y_shards[i])
        train_refs.append(ref)
        print(f"  â†’ æäº¤ Worker-{i} çš„è®­ç»ƒä»»åŠ¡")

    print("\n  â³ ç­‰å¾…æ‰€æœ‰ Worker å®Œæˆè®­ç»ƒ...")
    train_results = miniray.get(train_refs)

    print("\n  âœ… è®­ç»ƒç»“æœæ±‡æ€»:")
    total_time = max(r['train_time'] for r in train_results)
    avg_acc = np.mean([r['train_accuracy'] for r in train_results])
    print(f"      æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f}s (å¹¶è¡Œ)")
    print(f"      å¹³å‡è®­ç»ƒå‡†ç¡®ç‡: {avg_acc:.4f}")

    for result in train_results:
        print(f"      Worker-{result['worker_id']}: "
              f"è€—æ—¶ {result['train_time']:.2f}s, "
              f"å‡†ç¡®ç‡ {result['train_accuracy']:.4f}")

    print("\nğŸ“Œ ç¬¬ 5 æ­¥: ä½¿ç”¨æ‰€æœ‰æ¨¡å‹è¿›è¡Œé›†æˆé¢„æµ‹ï¼ˆæŠ•ç¥¨ï¼‰")
    predict_refs = []
    for worker in workers:
        ref = worker.predict.remote(X_test)
        predict_refs.append(ref)

    print("  â³ ç­‰å¾…æ‰€æœ‰ Worker å®Œæˆé¢„æµ‹...")
    all_predictions = miniray.get(predict_refs)

    # æŠ•ç¥¨é›†æˆï¼šæ¯ä¸ªæ ·æœ¬å–å¤šæ•°æŠ•ç¥¨
    all_predictions = np.array(all_predictions)  # shape: (num_workers, n_test_samples)
    ensemble_predictions = np.apply_along_axis(
        lambda x: np.bincount(x).argmax(),
        axis=0,
        arr=all_predictions
    )

    ensemble_acc = accuracy_score(y_test, ensemble_predictions)

    print(f"\n  âœ… é›†æˆæ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {ensemble_acc:.4f}")
    print("\n  ğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(y_test, ensemble_predictions,
                                target_names=[f'Class-{i}' for i in range(3)]))

    print("\nğŸ“Œ ç¬¬ 6 æ­¥: è·å–ç‰¹å¾é‡è¦æ€§")
    importance_refs = [worker.get_feature_importance.remote() for worker in workers]
    all_importance = miniray.get(importance_refs)

    # å¹³å‡ç‰¹å¾é‡è¦æ€§
    avg_importance = np.mean(all_importance, axis=0)
    top_5_features = np.argsort(avg_importance)[-5:][::-1]

    print("\n  âœ… Top 5 é‡è¦ç‰¹å¾:")
    for idx in top_5_features:
        print(f"      ç‰¹å¾ {idx}: é‡è¦æ€§ = {avg_importance[idx]:.4f}")

    print("\nğŸ“Œ ç¬¬ 7 æ­¥: å…³é—­æ‰€æœ‰ Worker")
    for i, worker in enumerate(workers):
        worker.shutdown()
        print(f"  âœ“ Worker-{i} å·²å…³é—­")


# ============================================================
# Demo 2: åˆ†å¸ƒå¼æ‰¹é‡æ¨ç†ï¼ˆDistributed Batch Inferenceï¼‰
# ============================================================

def demo_2_distributed_inference():
    """
    æ¼”ç¤º 2: åˆ†å¸ƒå¼æ‰¹é‡æ¨ç†

    åœºæ™¯ï¼šå·²æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œéœ€è¦å¯¹å¤§é‡æ•°æ®è¿›è¡Œæ¨ç†
    ç­–ç•¥ï¼šå°†æ¨ç†æ•°æ®åˆ†ç‰‡ï¼Œå¹¶è¡Œæ¨ç†ï¼Œæé«˜ååé‡
    """
    print_section("Demo 2: åˆ†å¸ƒå¼æ‰¹é‡æ¨ç†ï¼ˆDistributed Batch Inferenceï¼‰")

    try:
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import accuracy_score
    except ImportError:
        print("âš ï¸  éœ€è¦å®‰è£… scikit-learn: pip install scikit-learn")
        return

    # å®šä¹‰æ¨ç†æœåŠ¡ Actor
    @miniray.remote
    class InferenceServer:
        def __init__(self, server_id, model_params):
            self.server_id = server_id
            self.model = RandomForestClassifier(**model_params)
            self.request_count = 0
            print(f"  [InferenceServer-{server_id}] åˆå§‹åŒ–å®Œæˆ")

        def load_model(self, X_train, y_train):
            """åŠ è½½ï¼ˆè®­ç»ƒï¼‰æ¨¡å‹"""
            print(f"  [InferenceServer-{self.server_id}] è®­ç»ƒæ¨¡å‹...")
            self.model.fit(X_train, y_train)
            train_acc = self.model.score(X_train, y_train)
            print(f"  [InferenceServer-{self.server_id}] æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {train_acc:.4f}")
            return train_acc

        def predict_batch(self, X_batch):
            """æ‰¹é‡æ¨ç†"""
            self.request_count += 1
            start_time = time.time()

            predictions = self.model.predict(X_batch)
            probabilities = self.model.predict_proba(X_batch)

            inference_time = time.time() - start_time
            throughput = len(X_batch) / inference_time

            print(f"  [InferenceServer-{self.server_id}] "
                  f"æ¨ç†å®Œæˆ: {len(X_batch)} æ ·æœ¬, "
                  f"è€—æ—¶ {inference_time:.3f}s, "
                  f"ååé‡ {throughput:.0f} samples/s")

            return {
                'predictions': predictions,
                'probabilities': probabilities,
                'inference_time': inference_time,
                'throughput': throughput
            }

        def get_stats(self):
            """è·å–ç»Ÿè®¡ä¿¡æ¯"""
            return {
                'server_id': self.server_id,
                'request_count': self.request_count
            }

    print("\nğŸ“Œ ç¬¬ 1 æ­¥: ç”Ÿæˆæ•°æ®é›†")
    X_train, y_train = generate_synthetic_data(n_samples=5000, n_features=20, n_classes=3)
    X_inference, _ = generate_synthetic_data(n_samples=20000, n_features=20, n_classes=3,
                                             random_state=100)
    print(f"  è®­ç»ƒé›†: {X_train.shape}")
    print(f"  æ¨ç†æ•°æ®: {X_inference.shape}")

    print("\nğŸ“Œ ç¬¬ 2 æ­¥: åˆ›å»º 3 ä¸ªæ¨ç†æœåŠ¡å™¨")
    num_servers = 3
    model_params = {
        'n_estimators': 100,
        'max_depth': 10,
        'random_state': 42,
        'n_jobs': 1
    }

    servers = []
    for i in range(num_servers):
        server = InferenceServer.remote(server_id=i, model_params=model_params)
        servers.append(server)
        print(f"  â†’ åˆ›å»º InferenceServer-{i}")
    time.sleep(0.3)

    print("\nğŸ“Œ ç¬¬ 3 æ­¥: æ‰€æœ‰æœåŠ¡å™¨åŠ è½½æ¨¡å‹ï¼ˆè®­ç»ƒï¼‰")
    load_refs = [server.load_model.remote(X_train, y_train) for server in servers]
    load_results = miniray.get(load_refs)
    print(f"  âœ… æ‰€æœ‰æœåŠ¡å™¨æ¨¡å‹åŠ è½½å®Œæˆï¼Œå¹³å‡è®­ç»ƒå‡†ç¡®ç‡: {np.mean(load_results):.4f}")

    print("\nğŸ“Œ ç¬¬ 4 æ­¥: å°†æ¨ç†æ•°æ®åˆ†æˆæ‰¹æ¬¡")
    batch_size = 500
    num_batches = len(X_inference) // batch_size
    X_batches = [X_inference[i*batch_size:(i+1)*batch_size] for i in range(num_batches)]
    print(f"  æ€»æ‰¹æ¬¡æ•°: {num_batches}, æ¯æ‰¹å¤§å°: {batch_size}")

    print("\nğŸ“Œ ç¬¬ 5 æ­¥: åˆ†å¸ƒå¼æ‰¹é‡æ¨ç†ï¼ˆè½®è¯¢è°ƒåº¦ï¼‰")
    print("  â³ å¼€å§‹æ¨ç†...")

    inference_start = time.time()
    inference_refs = []

    # å°†æ‰¹æ¬¡è½®è¯¢åˆ†é…ç»™å„ä¸ªæœåŠ¡å™¨
    for i, batch in enumerate(X_batches):
        server = servers[i % num_servers]
        ref = server.predict_batch.remote(batch)
        inference_refs.append(ref)

    # æ”¶é›†æ‰€æœ‰ç»“æœ
    inference_results = miniray.get(inference_refs)
    total_inference_time = time.time() - inference_start

    print(f"\n  âœ… æ¨ç†å®Œæˆï¼")
    print(f"      æ€»è€—æ—¶: {total_inference_time:.2f}s")
    print(f"      æ€»æ ·æœ¬æ•°: {len(X_inference)}")
    print(f"      æ€»ååé‡: {len(X_inference) / total_inference_time:.0f} samples/s")

    # åˆå¹¶æ‰€æœ‰é¢„æµ‹ç»“æœ
    all_predictions = np.concatenate([r['predictions'] for r in inference_results])
    all_probabilities = np.concatenate([r['probabilities'] for r in inference_results])

    print(f"\n  ğŸ“Š é¢„æµ‹ç»“æœç»Ÿè®¡:")
    for cls in range(3):
        count = np.sum(all_predictions == cls)
        pct = count / len(all_predictions) * 100
        print(f"      Class-{cls}: {count} æ ·æœ¬ ({pct:.1f}%)")

    print("\nğŸ“Œ ç¬¬ 6 æ­¥: è·å–æœåŠ¡å™¨ç»Ÿè®¡ä¿¡æ¯")
    stats_refs = [server.get_stats.remote() for server in servers]
    stats = miniray.get(stats_refs)

    print("  ğŸ“Š æœåŠ¡å™¨è´Ÿè½½:")
    for stat in stats:
        print(f"      Server-{stat['server_id']}: å¤„ç†äº† {stat['request_count']} ä¸ªæ‰¹æ¬¡")

    print("\nğŸ“Œ ç¬¬ 7 æ­¥: å…³é—­æ‰€æœ‰æœåŠ¡å™¨")
    for i, server in enumerate(servers):
        server.shutdown()
        print(f"  âœ“ InferenceServer-{i} å·²å…³é—­")


# ============================================================
# Demo 3: å‚æ•°æœåŠ¡å™¨è®­ç»ƒï¼ˆParameter Server Trainingï¼‰
# ============================================================

def demo_3_parameter_server():
    """
    æ¼”ç¤º 3: å‚æ•°æœåŠ¡å™¨æ¨¡å¼è®­ç»ƒ

    è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆçš„å‚æ•°æœåŠ¡å™¨ï¼Œå±•ç¤ºåˆ†å¸ƒå¼æ¢¯åº¦ä¸‹é™çš„åŸºæœ¬æµç¨‹
    """
    print_section("Demo 3: å‚æ•°æœåŠ¡å™¨æ¨¡å¼ï¼ˆParameter Serverï¼‰")

    # å®šä¹‰å‚æ•°æœåŠ¡å™¨
    @miniray.remote
    class ParameterServer:
        def __init__(self, dim):
            self.params = np.zeros(dim)
            self.version = 0
            self.gradient_history = []
            print(f"  [ParameterServer] åˆå§‹åŒ–ï¼Œå‚æ•°ç»´åº¦: {dim}")

        def get_params(self):
            """è·å–å½“å‰å‚æ•°"""
            return self.params.copy(), self.version

        def update_params(self, gradients, learning_rate=0.01):
            """æ›´æ–°å‚æ•°"""
            self.params -= learning_rate * gradients
            self.version += 1
            self.gradient_history.append(np.linalg.norm(gradients))

            print(f"  [ParameterServer] å‚æ•°æ›´æ–° (v{self.version}), "
                  f"æ¢¯åº¦èŒƒæ•°: {np.linalg.norm(gradients):.4f}, "
                  f"å‚æ•°èŒƒæ•°: {np.linalg.norm(self.params):.4f}")

            return self.version

        def get_stats(self):
            """è·å–ç»Ÿè®¡ä¿¡æ¯"""
            return {
                'version': self.version,
                'param_norm': float(np.linalg.norm(self.params)),
                'param_mean': float(np.mean(self.params)),
                'param_std': float(np.std(self.params)),
                'gradient_history': self.gradient_history
            }

    # å®šä¹‰è®­ç»ƒ Worker
    @miniray.remote
    class GradientWorker:
        def __init__(self, worker_id, data_shard):
            self.worker_id = worker_id
            self.X, self.y = data_shard
            print(f"  [GradientWorker-{worker_id}] åˆå§‹åŒ–ï¼Œæ•°æ®é‡: {len(self.X)}")

        def compute_gradient(self, params):
            """
            è®¡ç®—æ¢¯åº¦ï¼ˆè¿™é‡Œä½¿ç”¨ç®€åŒ–çš„çº¿æ€§å›å½’æ¢¯åº¦ï¼‰
            æ¢¯åº¦ = (1/n) * X^T * (X*params - y)
            """
            n = len(self.X)
            predictions = self.X @ params
            errors = predictions - self.y
            gradient = (self.X.T @ errors) / n

            mse = np.mean(errors ** 2)

            print(f"  [GradientWorker-{self.worker_id}] "
                  f"è®¡ç®—æ¢¯åº¦å®Œæˆ, MSE: {mse:.4f}, "
                  f"æ¢¯åº¦èŒƒæ•°: {np.linalg.norm(gradient):.4f}")

            return gradient, mse

    print("\nğŸ“Œ ç¬¬ 1 æ­¥: ç”Ÿæˆçº¿æ€§å›å½’æ•°æ®")
    n_samples = 10000
    n_features = 50

    # ç”ŸæˆçœŸå®å‚æ•°
    true_params = np.random.randn(n_features) * 0.5

    # ç”Ÿæˆæ•°æ®
    X = np.random.randn(n_samples, n_features)
    y = X @ true_params + np.random.randn(n_samples) * 0.1  # åŠ å…¥å™ªå£°

    print(f"  æ•°æ®é›†: {X.shape}")
    print(f"  çœŸå®å‚æ•°èŒƒæ•°: {np.linalg.norm(true_params):.4f}")

    print("\nğŸ“Œ ç¬¬ 2 æ­¥: åˆ›å»ºå‚æ•°æœåŠ¡å™¨")
    ps = ParameterServer.remote(dim=n_features)
    time.sleep(0.2)

    print("\nğŸ“Œ ç¬¬ 3 æ­¥: åˆ›å»º 4 ä¸ªæ¢¯åº¦è®¡ç®— Worker")
    num_workers = 4

    # æ•°æ®åˆ†ç‰‡
    X_shards = np.array_split(X, num_workers)
    y_shards = np.array_split(y, num_workers)

    workers = []
    for i in range(num_workers):
        worker = GradientWorker.remote(
            worker_id=i,
            data_shard=(X_shards[i], y_shards[i])
        )
        workers.append(worker)
        print(f"  â†’ åˆ›å»º Worker-{i}, æ•°æ®é‡: {len(X_shards[i])}")
    time.sleep(0.3)

    print("\nğŸ“Œ ç¬¬ 4 æ­¥: å‚æ•°æœåŠ¡å™¨è®­ç»ƒå¾ªç¯")
    num_epochs = 10
    learning_rate = 0.01

    print(f"  å¼€å§‹è®­ç»ƒ: {num_epochs} è½®, å­¦ä¹ ç‡: {learning_rate}")

    for epoch in range(num_epochs):
        print(f"\n  --- Epoch {epoch + 1}/{num_epochs} ---")

        # 1. è·å–å½“å‰å‚æ•°
        params_ref = ps.get_params.remote()
        params, version = miniray.get(params_ref)
        print(f"    1ï¸âƒ£ è·å–å‚æ•° (v{version}), èŒƒæ•°: {np.linalg.norm(params):.4f}")

        # 2. æ‰€æœ‰ Worker å¹¶è¡Œè®¡ç®—æ¢¯åº¦
        gradient_refs = []
        for i, worker in enumerate(workers):
            ref = worker.compute_gradient.remote(params)
            gradient_refs.append(ref)

        print(f"    2ï¸âƒ£ Workers å¹¶è¡Œè®¡ç®—æ¢¯åº¦...")
        gradient_results = miniray.get(gradient_refs)

        gradients = [r[0] for r in gradient_results]
        mses = [r[1] for r in gradient_results]
        avg_mse = np.mean(mses)

        print(f"    3ï¸âƒ£ å¹³å‡ MSE: {avg_mse:.4f}")

        # 3. å¹³å‡æ¢¯åº¦
        avg_gradient = np.mean(gradients, axis=0)
        print(f"    4ï¸âƒ£ å¹³å‡æ¢¯åº¦èŒƒæ•°: {np.linalg.norm(avg_gradient):.4f}")

        # 4. æ›´æ–°å‚æ•°æœåŠ¡å™¨
        update_ref = ps.update_params.remote(avg_gradient, learning_rate)
        new_version = miniray.get(update_ref)

    print("\nğŸ“Œ ç¬¬ 5 æ­¥: è·å–æœ€ç»ˆç»Ÿè®¡")
    stats_ref = ps.get_stats.remote()
    stats = miniray.get(stats_ref)

    print("\n  âœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆç»Ÿè®¡:")
    print(f"      å‚æ•°ç‰ˆæœ¬: {stats['version']}")
    print(f"      å‚æ•°èŒƒæ•°: {stats['param_norm']:.4f}")
    print(f"      å‚æ•°å‡å€¼: {stats['param_mean']:.4f}")
    print(f"      å‚æ•°æ ‡å‡†å·®: {stats['param_std']:.4f}")

    print("\n  ğŸ“ˆ æ¢¯åº¦èŒƒæ•°å†å²:")
    for i, grad_norm in enumerate(stats['gradient_history'][:5]):
        print(f"      Epoch {i+1}: {grad_norm:.4f}")
    if len(stats['gradient_history']) > 5:
        print(f"      ...")
        for i, grad_norm in enumerate(stats['gradient_history'][-3:],
                                       start=len(stats['gradient_history'])-2):
            print(f"      Epoch {i}: {grad_norm:.4f}")

    print("\nğŸ“Œ ç¬¬ 6 æ­¥: å…³é—­æ‰€æœ‰ Actor")
    ps.shutdown()
    print("  âœ“ ParameterServer å·²å…³é—­")
    for i, worker in enumerate(workers):
        worker.shutdown()
        print(f"  âœ“ Worker-{i} å·²å…³é—­")


# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("  Mini-Ray ML è®­ç»ƒå’Œæ¨ç†æ¼”ç¤º")
    print("=" * 70)
    print("\næœ¬æ¼”ç¤ºå±•ç¤ºä¸‰ç§å¸¸è§çš„åˆ†å¸ƒå¼ ML æ¨¡å¼ï¼š")
    print("  1. æ•°æ®å¹¶è¡Œè®­ç»ƒ - å°†æ•°æ®åˆ†ç‰‡ï¼Œæ¯ä¸ª Worker è®­ç»ƒç‹¬ç«‹æ¨¡å‹")
    print("  2. åˆ†å¸ƒå¼æ‰¹é‡æ¨ç† - å°†æ¨ç†ä»»åŠ¡åˆ†é…ç»™å¤šä¸ªæœåŠ¡å™¨")
    print("  3. å‚æ•°æœåŠ¡å™¨ - é›†ä¸­å¼å‚æ•°ç®¡ç†ï¼Œåˆ†å¸ƒå¼æ¢¯åº¦è®¡ç®—")

    # åˆå§‹åŒ– Mini-Ray
    print("\n" + "=" * 70)
    print("  åˆå§‹åŒ– Mini-Ray")
    print("=" * 70)
    miniray.init(num_workers=4)
    time.sleep(0.5)

    try:
        # è¿è¡Œæ¼”ç¤º
        demo_1_data_parallel_training()
        demo_2_distributed_inference()
        demo_3_parameter_server()

        # æ€»ç»“
        print_section("æ¼”ç¤ºå®Œæˆ")
        print("\nâœ… æ‰€æœ‰ ML æ¼”ç¤ºéƒ½æˆåŠŸå®Œæˆï¼")
        print("\nğŸ“Š æ¼”ç¤ºå†…å®¹å›é¡¾:")
        print("  1. æ•°æ®å¹¶è¡Œè®­ç»ƒ - è®­ç»ƒ 4 ä¸ªæ¨¡å‹å‰¯æœ¬å¹¶é›†æˆ")
        print("  2. åˆ†å¸ƒå¼æ‰¹é‡æ¨ç† - 3 ä¸ªæœåŠ¡å™¨å¤„ç† 20000 æ ·æœ¬")
        print("  3. å‚æ•°æœåŠ¡å™¨ - 10 è½®è¿­ä»£è®­ç»ƒçº¿æ€§æ¨¡å‹")

        print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
        print("  â€¢ Actor æ¨¡å‹éå¸¸é€‚åˆæœ‰çŠ¶æ€çš„ ML ç»„ä»¶ï¼ˆæ¨¡å‹ã€å‚æ•°æœåŠ¡å™¨ï¼‰")
        print("  â€¢ å¹¶è¡ŒåŒ–å¯ä»¥æ˜¾è‘—æé«˜è®­ç»ƒå’Œæ¨ç†çš„ååé‡")
        print("  â€¢ å‚æ•°æœåŠ¡å™¨æ˜¯ç»å…¸çš„åˆ†å¸ƒå¼è®­ç»ƒæ¶æ„")

        print("\nğŸš€ ä¸‹ä¸€æ­¥:")
        print("  â€¢ å®ç° miniray.tune è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜")
        print("  â€¢ æ·»åŠ æ›´å¤æ‚çš„ ML æ¨¡å‹ï¼ˆç¥ç»ç½‘ç»œï¼‰")
        print("  â€¢ å®ç°æ¨¡å‹æ£€æŸ¥ç‚¹å’Œå®¹é”™æœºåˆ¶")

    finally:
        # å…³é—­ Mini-Ray
        print("\n" + "=" * 70)
        print("  å…³é—­ Mini-Ray")
        print("=" * 70)
        miniray.shutdown()


if __name__ == "__main__":
    main()
