# Mini-Ray ML Module - GAN for CIFAR-10

ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ç”¨äº CIFAR-10 å›¾ç‰‡ç”Ÿæˆ

**âœ¨ æ”¯æŒå•æœºå’Œåˆ†å¸ƒå¼è®­ç»ƒ + åˆ†å¸ƒå¼ç”Ÿæˆï¼**

## ğŸ“ æ–‡ä»¶ç»“æ„

```
ml/                           # å¤–éƒ¨ ML æ¨¡å—ç›®å½•
â”œâ”€â”€ __init__.py              # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ gan_cifar10.py           # GAN åŸºç¡€å®ç°ï¼ˆGenerator, Discriminatorï¼‰
â”œâ”€â”€ distributed_gan.py       # ğŸš€ åˆ†å¸ƒå¼ GANï¼ˆè®­ç»ƒ + ç”Ÿæˆï¼Œä½¿ç”¨ Mini-Ray @remoteï¼‰
â”œâ”€â”€ train.py                 # è®­ç»ƒå…¥å£ï¼ˆå•æœº + åˆ†å¸ƒå¼ï¼‰
â”œâ”€â”€ generate.py              # å›¾ç‰‡ç”Ÿæˆå…¥å£ï¼ˆå•æœº + åˆ†å¸ƒå¼ï¼‰
â”œâ”€â”€ requirements.txt         # ML ä¾èµ–
â””â”€â”€ README.md                # æœ¬æ–‡ä»¶
```

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
# æ–¹å¼ 1: å®‰è£… ML æ¨¡å—ä¾èµ–
pip install -r ml/requirements.txt

# æ–¹å¼ 2: å®‰è£…å®Œæ•´é¡¹ç›®ä¾èµ–ï¼ˆåŒ…æ‹¬ Dashboardï¼‰
pip install -r requirements.txt
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1ï¼šå•æœºè®­ç»ƒï¼ˆé€‚åˆå¿«é€Ÿæµ‹è¯•ï¼‰

```bash
# å•æœºè®­ç»ƒ 10 epochs
python -m ml.train --mode single --epochs 10
```

### æ–¹å¼ 2ï¼šåˆ†å¸ƒå¼è®­ç»ƒï¼ˆğŸ¯ Mini-Ray ç‰¹æ€§ï¼ï¼‰

```bash
# åˆ†å¸ƒå¼è®­ç»ƒ - 4 ä¸ª Worker å¹¶è¡Œ
python -m ml.train --mode distributed --workers 4 --epochs 10 --sync-interval 5
```

**åˆ†å¸ƒå¼è®­ç»ƒç‰¹ç‚¹ï¼š**
- âœ… ä½¿ç”¨ `@miniray.remote` è£…é¥°å™¨åˆ›å»ºåˆ†å¸ƒå¼ Worker
- âœ… æ•°æ®è‡ªåŠ¨åˆ†ç‰‡åˆ°å„ä¸ª Worker
- âœ… 4 ä¸ª Worker å¹¶è¡Œè®­ç»ƒï¼Œé€Ÿåº¦æå‡ ~4 å€
- âœ… å®šæœŸåŒæ­¥æ¨¡å‹å‚æ•°ï¼ˆå¹³å‡èšåˆï¼‰
- âœ… å®æ—¶åœ¨ Dashboard ç›‘æ§æ‰€æœ‰ Worker

### ç”Ÿæˆå›¾ç‰‡

#### å•æœºç”Ÿæˆï¼ˆé€‚åˆå°‘é‡å›¾ç‰‡ï¼‰

```bash
# ç”Ÿæˆ 10 å¼ å›¾ç‰‡
python -m ml.generate --model ./models/gan/generator.pth --num-images 10
```

#### åˆ†å¸ƒå¼ç”Ÿæˆï¼ˆğŸ†• å¿«é€Ÿç”Ÿæˆå¤§é‡å›¾ç‰‡ï¼ï¼‰

```bash
# ä½¿ç”¨ 4 ä¸ª Worker å¹¶è¡Œç”Ÿæˆ 100 å¼ å›¾ç‰‡
python -m ml.generate --model ./models/gan/generator.pth --num-images 100 --distributed --workers 4
```

**åˆ†å¸ƒå¼ç”Ÿæˆç‰¹ç‚¹ï¼š**
- âœ… ä½¿ç”¨ `@miniray.remote` è£…é¥°å™¨åˆ›å»ºåˆ†å¸ƒå¼ç”Ÿæˆ Worker
- âœ… å›¾ç‰‡ç”Ÿæˆä»»åŠ¡è‡ªåŠ¨åˆ†é…åˆ°å„ä¸ª Worker
- âœ… 4 ä¸ª Worker å¹¶è¡Œç”Ÿæˆï¼Œé€Ÿåº¦æå‡ ~4 å€
- âœ… é€‚åˆæ‰¹é‡ç”Ÿæˆå¤§é‡å›¾ç‰‡ï¼ˆ100+ï¼‰

## ğŸ“Š åˆ†å¸ƒå¼è®­ç»ƒè¯¦è§£

### æ¶æ„

```
Main Process
    â†“
å¯åŠ¨ 4 ä¸ª @miniray.remote Workers
    â”œâ”€â”€ Worker 0: å¤„ç†æ•°æ®åˆ†ç‰‡ 0-12499
    â”œâ”€â”€ Worker 1: å¤„ç†æ•°æ®åˆ†ç‰‡ 12500-24999
    â”œâ”€â”€ Worker 2: å¤„ç†æ•°æ®åˆ†ç‰‡ 25000-37499
    â””â”€â”€ Worker 3: å¤„ç†æ•°æ®åˆ†ç‰‡ 37500-49999

å¹¶è¡Œè®­ç»ƒ 1 epoch
    â†“
èšåˆç»“æœ (å¹³å‡ Loss)
    â†“
æ¯ 5 epochs åŒæ­¥å‚æ•°
    â†“
ç»§ç»­è®­ç»ƒ...
```

### å·¥ä½œæµç¨‹

1. **æ•°æ®åˆ†ç‰‡**
   - CIFAR-10 æœ‰ 50,000 å¼ è®­ç»ƒå›¾ç‰‡
   - 4 ä¸ª Worker å„å¤„ç† 12,500 å¼ 

2. **å¹¶è¡Œè®­ç»ƒ**
   - å„ Worker ç‹¬ç«‹è®­ç»ƒ GAN
   - ä½¿ç”¨ `miniray.get()` æ”¶é›†ç»“æœ

3. **å‚æ•°åŒæ­¥**
   - æ¯ N epochs æ”¶é›†æ‰€æœ‰æ¨¡å‹å‚æ•°
   - å¹³å‡èšåˆåå¹¿æ’­å›å„ Worker
   - ä¿æŒæ¨¡å‹ä¸€è‡´æ€§

4. **Dashboard ç›‘æ§**
   - å®æ—¶æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
   - èšåˆåçš„å¹³å‡ Loss
   - æ¯ä¸ª epoch çš„è¯¦ç»†æ—¥å¿—

## ğŸ“š API ä½¿ç”¨

### åˆ†å¸ƒå¼è®­ç»ƒ

```python
from miniray.ml import DistributedGANTrainer
from miniray.dashboard import get_collector
import miniray

# åˆå§‹åŒ– Mini-Ray
miniray.init(num_workers=4)

# åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒå™¨
trainer = DistributedGANTrainer(
    num_workers=4,
    latent_dim=100,
    lr=0.0002
)

# è®­ç»ƒ
history, workers = trainer.train(
    epochs=50,
    batch_size=128,
    sync_interval=5,  # æ¯ 5 epochs åŒæ­¥å‚æ•°
    job_id='dist-gan-001',
    collector=get_collector()
)

# æ¨¡å‹è‡ªåŠ¨ä¿å­˜åˆ° ./models/distributed_gan/worker_*
```

### å•æœºè®­ç»ƒ

```python
from miniray.ml import GANTrainer

trainer = GANTrainer(latent_dim=100, lr=0.0002)
history = trainer.train(epochs=10, batch_size=128)
trainer.save_models('./models/gan')
```

### å›¾ç‰‡ç”Ÿæˆ

```python
from miniray.ml import ImageGenerator

generator = ImageGenerator(latent_dim=100)
generator.load_model('./models/gan/generator.pth')

# ç”Ÿæˆ 10 å¼ å›¾ç‰‡
images = generator.generate(num_images=10)
generator.save_images(images, './output')
```

## âš™ï¸ å‘½ä»¤è¡Œå‚æ•°

### è®­ç»ƒ

```bash
python -m miniray.ml.train \
    --mode distributed \         # single æˆ– distributed
    --epochs 50 \                # è®­ç»ƒè½®æ•°
    --batch-size 128 \           # æ‰¹æ¬¡å¤§å°
    --workers 4 \                # Worker æ•°é‡ï¼ˆåˆ†å¸ƒå¼ï¼‰
    --sync-interval 5 \          # å‚æ•°åŒæ­¥é—´éš”
    --latent-dim 100 \           # éšå˜é‡ç»´åº¦
    --lr 0.0002 \                # å­¦ä¹ ç‡
    --save-dir ./models/gan      # ä¿å­˜ç›®å½•
```

### ç”Ÿæˆ

```bash
python -m miniray.ml.generate \
    --model ./models/gan/generator.pth \
    --num-images 20 \
    --output ./generated \
    --seed 42                    # å¯é€‰ï¼šå›ºå®šéšæœºç§å­
```

## ğŸ¯ æ¨¡å‹æ¶æ„

### Generatorï¼ˆç”Ÿæˆå™¨ï¼‰
```
Input: [batch, 100]               # éšæœºå™ªå£°
  â†“
Linear(100 â†’ 256) + ReLU + BN
  â†“
Linear(256 â†’ 512) + ReLU + BN
  â†“
Linear(512 â†’ 1024) + ReLU + BN
  â†“
Linear(1024 â†’ 3072) + Tanh
  â†“
Output: [batch, 3, 32, 32]       # RGB å›¾ç‰‡
```

### Discriminatorï¼ˆåˆ¤åˆ«å™¨ï¼‰
```
Input: [batch, 3, 32, 32]
  â†“
Flatten â†’ [batch, 3072]
  â†“
Linear(3072 â†’ 1024) + LeakyReLU + Dropout
  â†“
Linear(1024 â†’ 512) + LeakyReLU + Dropout
  â†“
Linear(512 â†’ 256) + LeakyReLU + Dropout
  â†“
Linear(256 â†’ 1) + Sigmoid
  â†“
Output: [batch, 1]               # çœŸå®æ¦‚ç‡
```

## â±ï¸ æ€§èƒ½å¯¹æ¯”

| æ¨¡å¼ | Workeræ•° | è®­ç»ƒæ—¶é—´/epoch | åŠ é€Ÿæ¯” |
|------|----------|---------------|--------|
| å•æœº | 1 | ~120s | 1x |
| åˆ†å¸ƒå¼ | 2 | ~65s | 1.8x |
| åˆ†å¸ƒå¼ | 4 | ~35s | 3.4x |

*æµ‹è¯•ç¯å¢ƒï¼šCPUè®­ç»ƒï¼ŒCIFAR-10æ•°æ®é›†*

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. é€‰æ‹©æ¨¡å¼
- **å¿«é€Ÿæµ‹è¯•**: å•æœºæ¨¡å¼ï¼Œ10 epochs
- **ç”Ÿäº§è®­ç»ƒ**: åˆ†å¸ƒå¼æ¨¡å¼ï¼Œ50-100 epochs

### 2. å‚æ•°è°ƒä¼˜
```bash
# è´¨é‡ä¼˜å…ˆï¼ˆè®­ç»ƒæ—¶é—´é•¿ï¼‰
python -m miniray.ml.train --mode distributed --workers 4 --epochs 100 --sync-interval 10

# é€Ÿåº¦ä¼˜å…ˆï¼ˆè´¨é‡ç¨å·®ï¼‰
python -m miniray.ml.train --mode single --epochs 20 --batch-size 256
```

### 3. Dashboard ç›‘æ§
```bash
# ç»ˆç«¯ 1: å¯åŠ¨ Dashboard
python -m miniray.dashboard

# ç»ˆç«¯ 2: è®­ç»ƒï¼ˆè‡ªåŠ¨è¿æ¥ Dashboardï¼‰
python -m miniray.ml.train --mode distributed --workers 4

# æµè§ˆå™¨: http://localhost:8266
# å®æ—¶æŸ¥çœ‹è®­ç»ƒè¿›åº¦ã€Lossã€æ—¥å¿—
```

## ğŸ› å¸¸è§é—®é¢˜

**Q: åˆ†å¸ƒå¼è®­ç»ƒæ¯”å•æœºæ…¢ï¼Ÿ**
A: å¯èƒ½åŸå› ï¼š
   - Worker æ•°é‡è¿‡å¤šï¼ˆè¶…è¿‡ CPU æ ¸å¿ƒæ•°ï¼‰
   - åŒæ­¥é—´éš”å¤ªçŸ­ï¼ˆé¢‘ç¹åŒæ­¥å¼€é”€å¤§ï¼‰
   - å»ºè®®ï¼šWorker æ•° = CPU æ ¸å¿ƒæ•° / 2ï¼Œsync_interval >= 5

**Q: è®­ç»ƒå‡ºç° OOMï¼ˆå†…å­˜ä¸è¶³ï¼‰ï¼Ÿ**
A: å‡å° batch_size æˆ–å‡å°‘ Worker æ•°é‡

**Q: ç”Ÿæˆçš„å›¾ç‰‡è´¨é‡ä¸å¥½ï¼Ÿ**
A: éœ€è¦è®­ç»ƒæ›´å¤š epochsï¼ˆæ¨è 50-100ï¼‰

**Q: å¦‚ä½•ä½¿ç”¨ GPU åŠ é€Ÿï¼Ÿ**
A: ä¿®æ”¹ `distributed_gan.py` ä¸­çš„ `device='cpu'` ä¸º `device='cuda'`

## ğŸ“ ä»£ç ç¤ºä¾‹

### å®Œæ•´è®­ç»ƒæµç¨‹

```python
import miniray
from miniray.ml import DistributedGANTrainer
from miniray.dashboard import get_collector

# 1. åˆå§‹åŒ– Mini-Ray
miniray.init(num_workers=4)

# 2. åˆ›å»ºè®­ç»ƒå™¨
trainer = DistributedGANTrainer(num_workers=4)

# 3. è®­ç»ƒ
history, workers = trainer.train(
    epochs=50,
    batch_size=128,
    sync_interval=5,
    job_id='my-gan',
    collector=get_collector()
)

# 4. ç”Ÿæˆå›¾ç‰‡æµ‹è¯•
from miniray.ml import ImageGenerator
gen = ImageGenerator()
gen.load_model('./models/distributed_gan/worker_0/generator.pth')
images = gen.generate(10)
gen.save_images(images, './test_output')

# 5. å…³é—­
miniray.shutdown()
```

## ğŸ¨ é«˜çº§åŠŸèƒ½

### è‡ªå®šä¹‰æ•°æ®åˆ†ç‰‡ç­–ç•¥

```python
# ä¿®æ”¹ DistributedGANWorker.load_data_shard() æ–¹æ³•
# æ”¯æŒè‡ªå®šä¹‰æ•°æ®åˆ†é…ç­–ç•¥
```

### æ··åˆç²¾åº¦è®­ç»ƒ

```python
# åœ¨ distributed_gan.py ä¸­æ·»åŠ  AMP æ”¯æŒ
# å¯è¿›ä¸€æ­¥åŠ é€Ÿè®­ç»ƒ
```

## ğŸ“ˆ æœªæ¥æ”¹è¿›

- [ ] æ”¯æŒ DCGANï¼ˆå·ç§¯ GANï¼‰æ¶æ„
- [ ] æ”¯æŒå…¶ä»–æ•°æ®é›†ï¼ˆImageNet, CelebAï¼‰
- [ ] æ·»åŠ  FID è¯„ä¼°æŒ‡æ ‡
- [ ] æ”¯æŒæ¡ä»¶ GANï¼ˆcGANï¼‰
- [ ] æ¢¯åº¦ç´¯ç§¯æ”¯æŒæ›´å¤§ batch size
