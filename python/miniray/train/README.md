# Mini-Ray Train - è½»é‡çº§åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶

é€šç”¨çš„åˆ†å¸ƒå¼è®­ç»ƒæŠ½è±¡ï¼Œé¿å…é‡å¤ç¼–å†™è®­ç»ƒå¾ªç¯ä»£ç ã€‚

## âœ¨ ç‰¹ç‚¹

- âœ… **é€šç”¨**ï¼šæ”¯æŒä»»æ„æ¨¡å‹ï¼ˆGANã€ResNetã€BERTã€åˆ†ç±»ã€å›å½’ç­‰ï¼‰
- âœ… **è½»é‡**ï¼š~200 è¡Œæ ¸å¿ƒä»£ç ï¼Œæ— é¢å¤–ä¾èµ–
- âœ… **ç®€æ´**ï¼šå°† 200+ è¡Œè®­ç»ƒä»£ç å‡å°‘åˆ° 20 è¡Œ
- âœ… **çµæ´»**ï¼šæ”¯æŒå®Œå…¨è‡ªå®šä¹‰è®­ç»ƒæµç¨‹
- âœ… **é›†æˆ**ï¼šè‡ªåŠ¨é›†æˆ ParameterServer è¿›è¡Œå‚æ•°åŒæ­¥

## ğŸ¯ è§£å†³çš„é—®é¢˜

### âŒ ä¿®æ”¹å‰ï¼šé‡å¤çš„è®­ç»ƒä»£ç 

æ¯æ¬¡å†™æ–°æ¨¡å‹éƒ½è¦é‡å¤è¿™äº›é€»è¾‘ï¼š

```python
# DistributedGANTrainerï¼ˆ~250 è¡Œï¼‰
class DistributedGANTrainer:
    def train(self, epochs):
        # 1. åˆå§‹åŒ– Mini-Ray
        miniray.init(num_workers=self.num_workers)

        # 2. åˆ›å»º Workers
        workers = [...]

        # 3. æ•°æ®åˆ†ç‰‡
        miniray.get([w.load_data_shard.remote(...)])

        # 4. è®­ç»ƒå¾ªç¯
        for epoch in range(epochs):
            results = miniray.get([w.train_epoch.remote(epoch) for w in workers])

            # 5. èšåˆç»“æœ
            avg_loss = np.mean([r["loss"] for r in results])

            # 6. å‚æ•°åŒæ­¥
            if (epoch + 1) % sync_interval == 0:
                # 15 è¡ŒåŒæ­¥ä»£ç ...

        # 7. æ¸…ç†
        miniray.shutdown()
```

**é—®é¢˜**ï¼š
- æ¯ä¸ªæ–°æ¨¡å‹éƒ½è¦å¤åˆ¶è¿™ 200+ è¡Œä»£ç 
- åŒæ ·çš„ bug ä¼šé‡å¤å‡ºç°ï¼ˆå‚æ•°é¡ºåºã€ç±»å‹å¤„ç†ã€åŒæ­¥æ—¶æœº...ï¼‰
- éš¾ä»¥ç»´æŠ¤å’Œå‡çº§

### âœ… ä¿®æ”¹åï¼šä½¿ç”¨ DataParallelTrainer

```python
# åªéœ€ 20 è¡Œï¼
class GANTrainer(DataParallelTrainer):
    def create_worker(self, worker_id, **kwargs):
        return GANWorker.remote(worker_id, self.latent_dim, self.lr)

    def on_epoch_end(self, epoch, result, **kwargs):
        print(f"G_loss: {result['g_loss']:.4f}, D_loss: {result['d_loss']:.4f}")

# ä½¿ç”¨
trainer = GANTrainer(num_workers=4, sync_interval=5)
trainer.train(epochs=50, batch_size=128)
```

**ä¼˜åŠ¿**ï¼š
- âœ… ä»£ç é‡å‡å°‘ **90%**
- âœ… æ•°æ®åˆ†ç‰‡ã€å‚æ•°åŒæ­¥ã€è®­ç»ƒå¾ªç¯å…¨éƒ¨è‡ªåŠ¨å¤„ç†
- âœ… Bug ä¿®å¤ä¸€æ¬¡ï¼Œæ‰€æœ‰æ¨¡å‹å—ç›Š
- âœ… æ˜“äºç»´æŠ¤å’Œæ‰©å±•

## ğŸ“¦ å¿«é€Ÿå¼€å§‹

### 1. å®šä¹‰ Workerï¼ˆéœ€å®ç° 3 ä¸ªæ–¹æ³•ï¼‰

```python
import miniray

@miniray.remote
class MyWorker:
    def __init__(self, worker_id):
        self.model = ...  # ä½ çš„æ¨¡å‹
        self.optimizer = ...

    # å¿…éœ€æ–¹æ³• 1ï¼šè®­ç»ƒä¸€ä¸ª epoch
    def train_epoch(self, epoch):
        # è®­ç»ƒé€»è¾‘...
        return {"loss": loss.item(), "accuracy": acc}

    # å¿…éœ€æ–¹æ³• 2ï¼šè·å–æ¨¡å‹å‚æ•°
    def get_weights(self):
        return [p.detach().cpu() for p in self.model.parameters()]

    # å¿…éœ€æ–¹æ³• 3ï¼šè®¾ç½®æ¨¡å‹å‚æ•°
    def set_weights(self, weights):
        with torch.no_grad():
            for param, new_weight in zip(self.model.parameters(), weights):
                param.copy_(new_weight)

    # å¯é€‰æ–¹æ³•ï¼šåŠ è½½æ•°æ®åˆ†ç‰‡
    def load_data_shard(self, shard_id, num_shards, **kwargs):
        # åŠ è½½ç¬¬ shard_id ä¸ªæ•°æ®åˆ†ç‰‡...
        pass
```

### 2. å®šä¹‰ Trainerï¼ˆç»§æ‰¿ DataParallelTrainerï¼‰

```python
from miniray.train import DataParallelTrainer

class MyTrainer(DataParallelTrainer):
    def create_worker(self, worker_id, **kwargs):
        return MyWorker.remote(worker_id)

    # å¯é€‰ï¼šè‡ªå®šä¹‰ epoch ç»“æŸå¤„ç†
    def on_epoch_end(self, epoch, result, **kwargs):
        super().on_epoch_end(epoch, result, **kwargs)
        print(f"Loss: {result['loss']:.4f}")
```

### 3. è®­ç»ƒï¼

```python
trainer = MyTrainer(
    num_workers=4,
    sync_interval=5,      # æ¯ 5 ä¸ª epoch åŒæ­¥å‚æ•°
    sync_strategy='average'  # å‚æ•°èšåˆç­–ç•¥
)

result = trainer.train(epochs=50, batch_size=128)
trainer.shutdown()
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ç±»å±‚æ¬¡ç»“æ„

```
BaseTrainer                    # åŸºç±»ï¼šç®¡ç†è®­ç»ƒå¾ªç¯ã€Worker ç”Ÿå‘½å‘¨æœŸ
    â”œâ”€â”€ DataParallelTrainer   # æ•°æ®å¹¶è¡Œï¼šè‡ªåŠ¨åˆ†ç‰‡ã€å‚æ•°åŒæ­¥
    â””â”€â”€ YourCustomTrainer     # è‡ªå®šä¹‰ï¼šå®Œå…¨æ§åˆ¶è®­ç»ƒæµç¨‹
```

### BaseTrainerï¼ˆåŸºç±»ï¼‰

èŒè´£ï¼š
- ç®¡ç† Mini-Ray ç”Ÿå‘½å‘¨æœŸï¼ˆinit/shutdownï¼‰
- Worker åˆ›å»ºå’Œç®¡ç†
- è®­ç»ƒå¾ªç¯æ¡†æ¶ï¼ˆepoch loopï¼‰
- é’©å­æ–¹æ³•ï¼ˆon_epoch_start, on_epoch_end ç­‰ï¼‰

**å…³é”®æ–¹æ³•**ï¼š

```python
class BaseTrainer:
    def train(self, epochs, **kwargs):
        """è®­ç»ƒä¸»å¾ªç¯"""

    def create_worker(self, worker_id, **kwargs):
        """åˆ›å»º Workerï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰"""

    def train_epoch(self, epoch, **kwargs):
        """è®­ç»ƒä¸€ä¸ª epochï¼ˆå­ç±»å¯é‡å†™ï¼‰"""

    # é’©å­æ–¹æ³•ï¼ˆå­ç±»å¯é€‰å®ç°ï¼‰
    def on_train_start(self, **kwargs): ...
    def on_train_end(self, history, **kwargs): ...
    def on_epoch_start(self, epoch, **kwargs): ...
    def on_epoch_end(self, epoch, result, **kwargs): ...
```

### DataParallelTrainerï¼ˆæ•°æ®å¹¶è¡Œï¼‰

èŒè´£ï¼š
- è‡ªåŠ¨æ•°æ®åˆ†ç‰‡ï¼ˆè°ƒç”¨ Worker çš„ `load_data_shard`ï¼‰
- å‚æ•°åŒæ­¥ï¼ˆä½¿ç”¨ ParameterServerï¼‰
- ç»“æœèšåˆï¼ˆå¯¹æ•°å€¼å­—æ®µå–å¹³å‡ï¼‰

**é¢å¤–å‚æ•°**ï¼š

```python
class DataParallelTrainer(BaseTrainer):
    def __init__(
        self,
        num_workers=4,
        sync_interval=1,           # å‚æ•°åŒæ­¥é—´éš”
        sync_strategy='average',   # èšåˆç­–ç•¥
        **strategy_kwargs          # ç­–ç•¥å‚æ•°
    ):
        ...
```

**é¢å¤–æ–¹æ³•**ï¼š

```python
def set_worker_weight(self, worker_id, weight):
    """è®¾ç½® Worker æƒé‡ï¼ˆç”¨äºåŠ æƒå¹³å‡ï¼‰"""

def get_sync_stats(self):
    """è·å–å‚æ•°åŒæ­¥ç»Ÿè®¡"""
```

## ğŸ“š å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1ï¼šGAN è®­ç»ƒ

```python
from miniray.train import DataParallelTrainer

class GANTrainer(DataParallelTrainer):
    def __init__(self, num_workers=4, latent_dim=100, lr=0.0002):
        super().__init__(num_workers=num_workers, sync_interval=5)
        self.latent_dim = latent_dim
        self.lr = lr

    def create_worker(self, worker_id, **kwargs):
        return GANWorker.remote(worker_id, self.latent_dim, self.lr)

    def on_epoch_end(self, epoch, result, **kwargs):
        super().on_epoch_end(epoch, result, **kwargs)
        print(f"  G_loss: {result['g_loss']:.4f}, D_loss: {result['d_loss']:.4f}")

# ä½¿ç”¨
trainer = GANTrainer(num_workers=4)
trainer.train(epochs=50, batch_size=128)
```

å®Œæ•´ä»£ç ï¼š`ml/distributed_gan_v2.py`

### ç¤ºä¾‹ 2ï¼šåˆ†ç±»æ¨¡å‹è®­ç»ƒ

```python
class ClassificationTrainer(DataParallelTrainer):
    def __init__(self, num_workers=4, lr=0.01):
        super().__init__(num_workers=num_workers, sync_interval=3)
        self.lr = lr

    def create_worker(self, worker_id, **kwargs):
        return ClassificationWorker.remote(worker_id, lr=self.lr)

    def on_epoch_end(self, epoch, result, **kwargs):
        super().on_epoch_end(epoch, result, **kwargs)
        print(f"  Accuracy: {result['accuracy']:.2%}")

# ä½¿ç”¨
trainer = ClassificationTrainer(num_workers=4)
trainer.train(epochs=20, dataset='cifar10')
```

å®Œæ•´ä»£ç ï¼š`examples/train_framework_demo.py`

### ç¤ºä¾‹ 3ï¼šè‡ªå®šä¹‰è®­ç»ƒæµç¨‹ï¼ˆBaseTrainerï¼‰

```python
from miniray.train import BaseTrainer

class CustomTrainer(BaseTrainer):
    def create_worker(self, worker_id, **kwargs):
        return CustomWorker.remote(worker_id)

    def train_epoch(self, epoch, **kwargs):
        # å®Œå…¨è‡ªå®šä¹‰è®­ç»ƒé€»è¾‘
        results = miniray.get([
            w.custom_method.remote(epoch) for w in self.workers
        ])
        return {"custom_metric": sum(results) / len(results)}

# ä½¿ç”¨
trainer = CustomTrainer(num_workers=4)
trainer.train(epochs=10)
```

### ç¤ºä¾‹ 4ï¼šä½¿ç”¨ with è¯­å¥

```python
with ClassificationTrainer(num_workers=4) as trainer:
    result = trainer.train(epochs=10)
    print("è®­ç»ƒå®Œæˆï¼")
# Mini-Ray è‡ªåŠ¨å…³é—­
```

## ğŸ¨ é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰ç»“æœèšåˆ

```python
class MyTrainer(DataParallelTrainer):
    def train_epoch(self, epoch, **kwargs):
        worker_results = miniray.get([
            w.train_epoch.remote(epoch) for w in self.workers
        ])

        # è‡ªå®šä¹‰èšåˆé€»è¾‘
        aggregated = {
            'loss': np.median([r['loss'] for r in worker_results]),  # ä¸­ä½æ•°
            'max_acc': max(r['accuracy'] for r in worker_results),   # æœ€å¤§å€¼
        }

        # å‚æ•°åŒæ­¥ï¼ˆç»§æ‰¿çš„é€»è¾‘ï¼‰
        if (epoch + 1) % self.sync_interval == 0:
            self.param_server.sync_from_workers.remote(self.workers)
            aggregated['synced'] = True

        return aggregated
```

### 2. åŠ æƒå¹³å‡ç­–ç•¥

```python
trainer = DataParallelTrainer(
    num_workers=4,
    sync_strategy='weighted'  # ä½¿ç”¨åŠ æƒå¹³å‡
)

# è®¾ç½®å„ Worker çš„æƒé‡ï¼ˆå¦‚æ ·æœ¬æ•°é‡ï¼‰
trainer.set_worker_weight(0, 1000)
trainer.set_worker_weight(1, 2000)
trainer.set_worker_weight(2, 1500)
trainer.set_worker_weight(3, 2500)

trainer.train(epochs=50)
```

### 3. åŠ¨é‡ç­–ç•¥

```python
trainer = DataParallelTrainer(
    num_workers=4,
    sync_strategy='momentum',
    momentum=0.9  # ç­–ç•¥å‚æ•°
)

trainer.train(epochs=50)
```

### 4. é’©å­æ–¹æ³•ä½¿ç”¨

```python
class MyTrainer(DataParallelTrainer):
    def on_train_start(self, **kwargs):
        print("è®­ç»ƒå¼€å§‹ï¼Œåˆå§‹åŒ–...")
        # è‡ªå®šä¹‰åˆå§‹åŒ–é€»è¾‘

    def on_epoch_start(self, epoch, **kwargs):
        print(f"Epoch {epoch} å¼€å§‹")
        # Epoch å¼€å§‹æ—¶çš„æ“ä½œ

    def on_epoch_end(self, epoch, result, **kwargs):
        super().on_epoch_end(epoch, result, **kwargs)
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 10 == 0:
            self.save_checkpoint(epoch)

    def on_train_end(self, history, **kwargs):
        result = super().on_train_end(history, **kwargs)
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_final_model()
        return result
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. è°ƒæ•´åŒæ­¥é—´éš”

```python
# é¢‘ç¹åŒæ­¥ï¼šå‚æ•°æ›´ä¸€è‡´ï¼Œä½†å¼€é”€å¤§
trainer = DataParallelTrainer(sync_interval=1)

# ç¨€ç–åŒæ­¥ï¼šå¼€é”€å°ï¼Œä½†å‚æ•°å¯èƒ½å‘æ•£
trainer = DataParallelTrainer(sync_interval=10)

# æ¨èï¼š5-10
trainer = DataParallelTrainer(sync_interval=5)
```

### 2. Worker æ•°é‡

```python
# Worker æ•°é‡ = CPU æ ¸å¿ƒæ•° / 2ï¼ˆæ¨èï¼‰
import os
num_workers = os.cpu_count() // 2

trainer = DataParallelTrainer(num_workers=num_workers)
```

### 3. æ‰¹é‡å¤§å°

```python
# æ€»æ‰¹é‡å¤§å° = batch_size * num_workers
# å»ºè®®ä¿æŒæ€»æ‰¹é‡å¤§å°ä¸å˜ï¼Œè°ƒæ•´ batch_size
total_batch = 512
num_workers = 4
batch_size = total_batch // num_workers  # 128

trainer.train(epochs=50, batch_size=batch_size)
```

## ğŸ”„ ä¸åŸå§‹ä»£ç å¯¹æ¯”

| ç‰¹æ€§ | åŸå§‹ä»£ç ï¼ˆDistributedGANTrainerï¼‰ | ä½¿ç”¨ Train æ¡†æ¶ |
|------|----------------------------------|-----------------|
| ä»£ç é‡ | ~250 è¡Œ | ~20 è¡Œ |
| æ•°æ®åˆ†ç‰‡ | æ‰‹åŠ¨å®ç° | è‡ªåŠ¨å¤„ç† |
| å‚æ•°åŒæ­¥ | æ‰‹åŠ¨å®ç°ï¼ˆ15 è¡Œï¼‰ | è‡ªåŠ¨å¤„ç†ï¼ˆ1 è¡Œï¼‰|
| ç»“æœèšåˆ | æ‰‹åŠ¨å®ç° | è‡ªåŠ¨å¤„ç† |
| è®­ç»ƒå¾ªç¯ | æ‰‹åŠ¨å®ç° | è‡ªåŠ¨å¤„ç† |
| ç”Ÿå‘½å‘¨æœŸç®¡ç† | æ‰‹åŠ¨ init/shutdown | è‡ªåŠ¨ç®¡ç† |
| å¯æ‰©å±•æ€§ | ä½ï¼ˆéœ€å¤åˆ¶ä»£ç ï¼‰| é«˜ï¼ˆç»§æ‰¿æ‰©å±•ï¼‰|
| å¯ç»´æŠ¤æ€§ | ä½ï¼ˆåˆ†æ•£åœ¨å„å¤„ï¼‰| é«˜ï¼ˆé›†ä¸­ç®¡ç†ï¼‰|

## ğŸ› å¸¸è§é—®é¢˜

**Q: Worker å¿…é¡»å®ç°å“ªäº›æ–¹æ³•ï¼Ÿ**

A:
- å¿…éœ€ï¼š`train_epoch(epoch)`, `get_weights()`, `set_weights(weights)`
- å¯é€‰ï¼š`load_data_shard(shard_id, num_shards, **kwargs)`

**Q: å¦‚ä½•ä¼ é€’å‚æ•°ç»™ Workerï¼Ÿ**

A: é€šè¿‡ `create_worker()` æ–¹æ³•ï¼š

```python
def create_worker(self, worker_id, **kwargs):
    lr = kwargs.get('lr', 0.01)
    return MyWorker.remote(worker_id, lr=lr)

trainer.train(epochs=50, lr=0.001)  # ä¼ é€’ç»™ create_worker
```

**Q: å¦‚ä½•è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Ÿ**

A: ç»§æ‰¿ BaseTrainer å¹¶é‡å†™ `train_epoch()` æ–¹æ³•ã€‚

**Q: æ”¯æŒå¤š GPU è®­ç»ƒå—ï¼Ÿ**

A: æ”¯æŒã€‚åœ¨ Worker ä¸­æŒ‡å®š deviceï¼Œå‚æ•°åŒæ­¥åœ¨ CPU ä¸Šè¿›è¡Œã€‚

**Q: å¦‚ä½•ä¿å­˜æ¨¡å‹ï¼Ÿ**

A: åœ¨ `on_epoch_end()` æˆ– `on_train_end()` ä¸­è°ƒç”¨ Worker çš„ä¿å­˜æ–¹æ³•ã€‚

## ğŸ“ˆ è·¯çº¿å›¾

- [x] BaseTrainer - é€šç”¨è®­ç»ƒå¾ªç¯
- [x] DataParallelTrainer - æ•°æ®å¹¶è¡Œ
- [x] ParameterServer é›†æˆ
- [ ] ModelParallelTrainer - æ¨¡å‹å¹¶è¡Œï¼ˆæœªæ¥ï¼‰
- [ ] PipelineParallelTrainer - æµæ°´çº¿å¹¶è¡Œï¼ˆæœªæ¥ï¼‰
- [ ] æ£€æŸ¥ç‚¹å’Œæ¢å¤ï¼ˆæœªæ¥ï¼‰
- [ ] åˆ†å¸ƒå¼è¯„ä¼°ï¼ˆæœªæ¥ï¼‰

## ğŸ“– æ›´å¤šèµ„æº

- **å®Œæ•´ç¤ºä¾‹**: `examples/train_framework_demo.py`
- **GAN è®­ç»ƒ**: `ml/distributed_gan_v2.py`
- **ParameterServer æ–‡æ¡£**: `python/miniray/ps/README.md`

## ğŸ“„ è®¸å¯è¯

MIT License
