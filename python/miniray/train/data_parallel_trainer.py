"""
Data Parallel Trainer - æ•°æ®å¹¶è¡Œè®­ç»ƒå™¨

å°è£…ã€Œå¤š Worker + å‘¨æœŸæ€§å‚æ•°å¹³å‡ã€çš„æ•°æ®å¹¶è¡Œè®­ç»ƒæ¨¡å¼
"""

from typing import List, Dict, Any, Optional, Callable
import numpy as np

import miniray
from miniray.ps import create_parameter_server
from miniray.train.base_trainer import BaseTrainer


class DataParallelTrainer(BaseTrainer):
    """
    æ•°æ®å¹¶è¡Œè®­ç»ƒå™¨

    ç‰¹ç‚¹ï¼š
    1. æ•°æ®è‡ªåŠ¨åˆ†ç‰‡åˆ°å„ä¸ª Worker
    2. å‘¨æœŸæ€§å‚æ•°åŒæ­¥ï¼ˆä½¿ç”¨ ParameterServerï¼‰
    3. ç»“æœè‡ªåŠ¨èšåˆ
    4. æ”¯æŒè‡ªå®šä¹‰èšåˆç­–ç•¥

    ä½¿ç”¨åœºæ™¯ï¼š
    - æ•°æ®å¹¶è¡Œè®­ç»ƒï¼ˆPyTorch DDPã€Horovod é£æ ¼ï¼‰
    - å®šæœŸåŒæ­¥æ¨¡å‹å‚æ•°
    - æ”¯æŒä»»æ„æ¨¡å‹ï¼ˆGANã€ResNetã€BERT ç­‰ï¼‰

    Worker æ¥å£è¦æ±‚ï¼š
        - train_epoch(epoch) -> Dict: è®­ç»ƒä¸€ä¸ª epochï¼Œè¿”å›æŒ‡æ ‡
        - get_weights() -> List[Tensor]: è·å–æ¨¡å‹å‚æ•°
        - set_weights(weights) -> None: è®¾ç½®æ¨¡å‹å‚æ•°
        - load_data_shard(shard_id, num_shards, **kwargs) -> None: åŠ è½½æ•°æ®åˆ†ç‰‡ï¼ˆå¯é€‰ï¼‰
    """

    def __init__(
        self,
        num_workers: int = 4,
        sync_interval: int = 1,
        sync_strategy: str = 'average',
        auto_init: bool = True,
        **strategy_kwargs
    ):
        """
        Args:
            num_workers: Worker æ•°é‡
            sync_interval: å‚æ•°åŒæ­¥é—´éš”ï¼ˆæ¯ N ä¸ª epoch åŒæ­¥ä¸€æ¬¡ï¼‰
            sync_strategy: å‚æ•°èšåˆç­–ç•¥ ('average', 'weighted', 'momentum')
            auto_init: æ˜¯å¦è‡ªåŠ¨åˆå§‹åŒ– Mini-Ray
            **strategy_kwargs: ç­–ç•¥ç‰¹å®šå‚æ•°
        """
        super().__init__(num_workers=num_workers, auto_init=auto_init)

        self.sync_interval = sync_interval
        self.sync_strategy = sync_strategy
        self.strategy_kwargs = strategy_kwargs

        self.param_server = None

        print(f"[DataParallelTrainer] é…ç½®:")
        print(f"  Workers: {num_workers}")
        print(f"  Sync Interval: {sync_interval} epochs")
        print(f"  Sync Strategy: {sync_strategy}")

    # ============================================================
    # è®­ç»ƒæµç¨‹é‡å†™
    # ============================================================

    def on_train_start(self, **kwargs):
        """è®­ç»ƒå¼€å§‹ï¼šåˆ›å»º ParameterServer å’Œæ•°æ®åˆ†ç‰‡"""
        print("[DataParallelTrainer] åˆ›å»º ParameterServer...")
        self.param_server = create_parameter_server(
            self.sync_strategy,
            **self.strategy_kwargs
        )

        # æ•°æ®åˆ†ç‰‡ï¼ˆå¦‚æœ Worker æ”¯æŒï¼‰
        print("[DataParallelTrainer] åŠ è½½æ•°æ®åˆ†ç‰‡...")
        self._load_data_shards(**kwargs)

    def train_epoch(self, epoch: int, **kwargs) -> Dict[str, Any]:
        """
        è®­ç»ƒä¸€ä¸ª epochï¼Œå¹¶åœ¨éœ€è¦æ—¶åŒæ­¥å‚æ•°

        Args:
            epoch: å½“å‰ epoch ç¼–å·

        Returns:
            Epoch ç»“æœï¼ŒåŒ…æ‹¬èšåˆåçš„æŒ‡æ ‡
        """
        # 1. å¹¶è¡Œè®­ç»ƒ
        worker_results = miniray.get([
            w.train_epoch.remote(epoch) for w in self.workers
        ])

        # 2. èšåˆç»“æœ
        aggregated = self._aggregate_results(worker_results)

        # 3. å‚æ•°åŒæ­¥
        if (epoch + 1) % self.sync_interval == 0:
            print(f"  ğŸ”„ åŒæ­¥å‚æ•° (epoch {epoch + 1})...")
            self.param_server.sync_from_workers.remote(self.workers)

            # è·å–åŒæ­¥ç»Ÿè®¡
            stats = miniray.get(self.param_server.get_stats.remote())
            aggregated['sync_version'] = stats['version']
            aggregated['synced'] = True
        else:
            aggregated['synced'] = False

        aggregated['worker_results'] = worker_results
        return aggregated

    def on_epoch_end(self, epoch: int, result: Dict, **kwargs):
        """æ¯ä¸ª epoch ç»“æŸï¼šæ‰“å°èšåˆåçš„æŒ‡æ ‡"""
        elapsed = result.get('time', 0)
        synced = "âœ… å·²åŒæ­¥" if result.get('synced', False) else ""

        print(f"[DataParallelTrainer] Epoch {epoch + 1} å®Œæˆ (è€—æ—¶: {elapsed:.2f}s) {synced}")

        # æ‰“å°èšåˆæŒ‡æ ‡
        for key, value in result.items():
            if key not in ['worker_results', 'epoch', 'time', 'synced', 'sync_version']:
                if isinstance(value, (int, float)):
                    print(f"  {key}: {value:.4f}")

    # ============================================================
    # æ•°æ®åˆ†ç‰‡
    # ============================================================

    def _load_data_shards(self, **kwargs):
        """
        åŠ è½½æ•°æ®åˆ†ç‰‡ï¼ˆå¦‚æœ Worker æ”¯æŒ load_data_shard æ–¹æ³•ï¼‰

        Args:
            **kwargs: ä¼ é€’ç»™ load_data_shard çš„å‚æ•°ï¼ˆå¦‚ batch_size, dataset_path ç­‰ï¼‰
        """
        # æ£€æŸ¥ Worker æ˜¯å¦æœ‰ load_data_shard æ–¹æ³•
        # é€šè¿‡å°è¯•è°ƒç”¨æ¥åˆ¤æ–­ï¼ˆå¦‚æœä¸æ”¯æŒä¼šæŠ¥é”™ï¼Œä½†æˆ‘ä»¬æ•è·å®ƒï¼‰
        try:
            miniray.get([
                w.load_data_shard.remote(i, self.num_workers, **kwargs)
                for i, w in enumerate(self.workers)
            ])
            print(f"  âœ… æ•°æ®å·²åˆ†ç‰‡åˆ° {self.num_workers} ä¸ª Workers")
        except Exception as e:
            # Worker ä¸æ”¯æŒ load_data_shardï¼Œè·³è¿‡
            print(f"  â„¹ï¸  Workers ä¸æ”¯æŒè‡ªåŠ¨æ•°æ®åˆ†ç‰‡ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰")

    # ============================================================
    # ç»“æœèšåˆ
    # ============================================================

    def _aggregate_results(
        self,
        worker_results: List[Dict],
        aggregation_fn: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """
        èšåˆæ‰€æœ‰ Worker çš„è®­ç»ƒç»“æœ

        Args:
            worker_results: æ‰€æœ‰ Worker çš„ç»“æœåˆ—è¡¨
            aggregation_fn: è‡ªå®šä¹‰èšåˆå‡½æ•°ï¼ˆå¯é€‰ï¼‰

        Returns:
            èšåˆåçš„ç»“æœå­—å…¸

        Note:
            é»˜è®¤å¯¹æ•°å€¼ç±»å‹å­—æ®µå–å¹³å‡å€¼
        """
        if aggregation_fn:
            return aggregation_fn(worker_results)

        # é»˜è®¤èšåˆç­–ç•¥ï¼šå¯¹æ•°å€¼å­—æ®µå–å¹³å‡
        aggregated = {}

        if not worker_results:
            return aggregated

        # è·å–æ‰€æœ‰å­—æ®µ
        keys = set()
        for result in worker_results:
            if isinstance(result, dict):
                keys.update(result.keys())

        # å¯¹æ¯ä¸ªå­—æ®µèšåˆ
        for key in keys:
            values = []
            for result in worker_results:
                if isinstance(result, dict) and key in result:
                    values.append(result[key])

            if not values:
                continue

            # æ•°å€¼ç±»å‹ï¼šå–å¹³å‡
            if isinstance(values[0], (int, float)):
                aggregated[key] = float(np.mean(values))
            # å…¶ä»–ç±»å‹ï¼šå–ç¬¬ä¸€ä¸ª
            else:
                aggregated[key] = values[0]

        return aggregated

    # ============================================================
    # è¾…åŠ©æ–¹æ³•
    # ============================================================

    def set_worker_weight(self, worker_id: int, weight: float):
        """
        è®¾ç½® Worker æƒé‡ï¼ˆç”¨äºåŠ æƒå¹³å‡ç­–ç•¥ï¼‰

        Args:
            worker_id: Worker ID
            weight: æƒé‡å€¼ï¼ˆå¦‚æ ·æœ¬æ•°é‡ï¼‰
        """
        if self.param_server:
            miniray.get(self.param_server.set_worker_weight.remote(worker_id, weight))

    def get_sync_stats(self) -> Dict:
        """è·å–å‚æ•°åŒæ­¥ç»Ÿè®¡ä¿¡æ¯"""
        if self.param_server:
            return miniray.get(self.param_server.get_stats.remote())
        return {}
