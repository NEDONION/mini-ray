#!/usr/bin/env python3
"""
Mini-Ray è¶…å‚æ•°è°ƒä¼˜ Demo

å±•ç¤ºå¦‚ä½•ä½¿ç”¨ miniray.tune è¿›è¡Œè¶…å‚æ•°æœç´¢ï¼š
1. ç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰
2. éšæœºæœç´¢ï¼ˆRandom Searchï¼‰
3. å®é™… ML æ¨¡å‹è°ƒä¼˜ï¼ˆä½¿ç”¨ sklearnï¼‰

è¿™æ˜¯ Phase 3 çš„æ ¸å¿ƒåŠŸèƒ½ä¹‹ä¸€ï¼
"""
import sys
import os
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, PROJECT_ROOT)

import miniray
from miniray import tune


def print_section(title):
    """æ‰“å°åˆ†éš”çº¿å’Œæ ‡é¢˜"""
    print("\n" + "=" * 70)
    print(f"  {title}")
    print("=" * 70)


# ============================================================
# Demo 1: ç®€å•çš„ç½‘æ ¼æœç´¢
# ============================================================

def demo_1_simple_grid_search():
    """
    æ¼”ç¤º 1: ç®€å•çš„ç½‘æ ¼æœç´¢

    ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥å±•ç¤º tune çš„åŸºæœ¬ç”¨æ³•
    """
    print_section("Demo 1: ç®€å•çš„ç½‘æ ¼æœç´¢")

    def simple_objective(config):
        """
        ç®€å•çš„ç›®æ ‡å‡½æ•°

        Args:
            config: è¶…å‚æ•°é…ç½®

        Returns:
            åŒ…å« score çš„å­—å…¸
        """
        x = config['x']
        y = config['y']

        # ç›®æ ‡ï¼šæœ€å¤§åŒ– -(x-3)^2 - (y-5)^2 + 10
        # æœ€ä¼˜è§£ï¼šx=3, y=5, score=10
        score = -(x - 3) ** 2 - (y - 5) ** 2 + 10

        # æ¨¡æ‹Ÿè®¡ç®—æ—¶é—´
        time.sleep(0.05)

        return {'score': score, 'x': x, 'y': y}

    print("\nğŸ“Œ æœç´¢ç©ºé—´:")
    print("  x: [0, 1, 2, 3, 4, 5, 6]")
    print("  y: [2, 3, 4, 5, 6, 7, 8]")
    print(f"  æ€»é…ç½®æ•°: 7 Ã— 7 = 49")
    print("\nğŸ“Œ ç›®æ ‡å‡½æ•°: æœ€å¤§åŒ– -(x-3)Â² - (y-5)Â² + 10")
    print("  ç†è®ºæœ€ä¼˜è§£: x=3, y=5, score=10")
    print("")

    # è¿è¡Œè°ƒä¼˜
    analysis = tune.run(
        simple_objective,
        config={
            'x': tune.grid_search([0, 1, 2, 3, 4, 5, 6]),
            'y': tune.grid_search([2, 3, 4, 5, 6, 7, 8]),
        },
        metric='score',
        mode='max',
        verbose=True
    )

    print("\nğŸ“Š è°ƒä¼˜ç»“æœ:")
    print(f"  æœ€ä½³é…ç½®: {analysis.best_config}")
    print(f"  æœ€ä½³å¾—åˆ†: {analysis.best_result['score']:.4f}")
    print(f"  æˆåŠŸè¯•éªŒæ•°: {len(analysis.successful_trials)}")


# ============================================================
# Demo 2: éšæœºæ£®æ—è¶…å‚æ•°è°ƒä¼˜
# ============================================================

def demo_2_random_forest_tuning():
    """
    æ¼”ç¤º 2: éšæœºæ£®æ—è¶…å‚æ•°è°ƒä¼˜

    ä½¿ç”¨çœŸå®çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œè¶…å‚æ•°æœç´¢
    """
    print_section("Demo 2: éšæœºæ£®æ—è¶…å‚æ•°è°ƒä¼˜")

    try:
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.datasets import load_digits
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score
    except ImportError:
        print("âš ï¸  éœ€è¦å®‰è£… scikit-learn: pip install scikit-learn")
        return

    print("\nğŸ“Œ ç¬¬ 1 æ­¥: åŠ è½½æ•°æ®é›†ï¼ˆMNIST æ‰‹å†™æ•°å­—ï¼‰")
    X, y = load_digits(return_X_y=True)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    print(f"  è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    def train_random_forest(config):
        """
        è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹

        Args:
            config: è¶…å‚æ•°é…ç½®

        Returns:
            åŒ…å« accuracy çš„å­—å…¸
        """
        # åˆ›å»ºæ¨¡å‹
        model = RandomForestClassifier(
            n_estimators=config['n_estimators'],
            max_depth=config['max_depth'],
            min_samples_split=config['min_samples_split'],
            random_state=42,
            n_jobs=1
        )

        # è®­ç»ƒ
        start_time = time.time()
        model.fit(X_train, y_train)
        train_time = time.time() - start_time

        # è¯„ä¼°
        train_acc = accuracy_score(y_train, model.predict(X_train))
        test_acc = accuracy_score(y_test, model.predict(X_test))

        return {
            'accuracy': test_acc,
            'train_accuracy': train_acc,
            'train_time': train_time,
        }

    print("\nğŸ“Œ ç¬¬ 2 æ­¥: å®šä¹‰æœç´¢ç©ºé—´")
    print("  n_estimators: [50, 100, 200]")
    print("  max_depth: [5, 10, 20, None]")
    print("  min_samples_split: [2, 5, 10]")
    print(f"  æ€»é…ç½®æ•°: 3 Ã— 4 Ã— 3 = 36")
    print("")

    # è¿è¡Œè°ƒä¼˜
    analysis = tune.run(
        train_random_forest,
        config={
            'n_estimators': tune.grid_search([50, 100, 200]),
            'max_depth': tune.grid_search([5, 10, 20, None]),
            'min_samples_split': tune.grid_search([2, 5, 10]),
        },
        metric='accuracy',
        mode='max',
        verbose=True
    )

    print("\nğŸ“Š è¯¦ç»†ç»“æœåˆ†æ:")
    print(f"  æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {analysis.best_result['accuracy']:.4f}")
    print(f"  æœ€ä½³è®­ç»ƒå‡†ç¡®ç‡: {analysis.best_result['train_accuracy']:.4f}")
    print(f"  è®­ç»ƒè€—æ—¶: {analysis.best_result['train_time']:.2f}s")
    print(f"  æœ€ä½³é…ç½®:")
    for key, value in analysis.best_config.items():
        print(f"    {key}: {value}")


# ============================================================
# Demo 3: ç¥ç»ç½‘ç»œè¶…å‚æ•°è°ƒä¼˜ï¼ˆç®€åŒ–ç‰ˆï¼‰
# ============================================================

def demo_3_neural_network_tuning():
    """
    æ¼”ç¤º 3: ç¥ç»ç½‘ç»œè¶…å‚æ•°è°ƒä¼˜

    ä½¿ç”¨ sklearn çš„ MLPClassifier æ¨¡æ‹Ÿç¥ç»ç½‘ç»œè°ƒä¼˜
    """
    print_section("Demo 3: ç¥ç»ç½‘ç»œè¶…å‚æ•°è°ƒä¼˜")

    try:
        from sklearn.neural_network import MLPClassifier
        from sklearn.datasets import make_classification
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score
        from sklearn.preprocessing import StandardScaler
    except ImportError:
        print("âš ï¸  éœ€è¦å®‰è£… scikit-learn: pip install scikit-learn")
        return

    print("\nğŸ“Œ ç¬¬ 1 æ­¥: ç”Ÿæˆåˆæˆæ•°æ®é›†")
    X, y = make_classification(
        n_samples=2000,
        n_features=20,
        n_informative=15,
        n_redundant=5,
        n_classes=3,
        random_state=42
    )

    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42
    )
    print(f"  è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    def train_mlp(config):
        """
        è®­ç»ƒç¥ç»ç½‘ç»œ

        Args:
            config: è¶…å‚æ•°é…ç½®

        Returns:
            åŒ…å« accuracy çš„å­—å…¸
        """
        # åˆ›å»ºæ¨¡å‹
        model = MLPClassifier(
            hidden_layer_sizes=config['hidden_layers'],
            learning_rate_init=config['learning_rate'],
            batch_size=config['batch_size'],
            max_iter=200,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1,
            verbose=False
        )

        # è®­ç»ƒ
        start_time = time.time()
        model.fit(X_train, y_train)
        train_time = time.time() - start_time

        # è¯„ä¼°
        test_acc = accuracy_score(y_test, model.predict(X_test))
        train_acc = accuracy_score(y_train, model.predict(X_train))

        return {
            'accuracy': test_acc,
            'train_accuracy': train_acc,
            'train_time': train_time,
            'n_iter': model.n_iter_,
        }

    print("\nğŸ“Œ ç¬¬ 2 æ­¥: å®šä¹‰æœç´¢ç©ºé—´")
    print("  hidden_layers: [(50,), (100,), (50, 50), (100, 50)]")
    print("  learning_rate: [0.001, 0.01, 0.1]")
    print("  batch_size: [32, 64, 128]")
    print(f"  æ€»é…ç½®æ•°: 4 Ã— 3 Ã— 3 = 36")
    print("")

    # è¿è¡Œè°ƒä¼˜
    analysis = tune.run(
        train_mlp,
        config={
            'hidden_layers': tune.grid_search([
                (50,),      # å•éšè—å±‚ 50 ä¸ªç¥ç»å…ƒ
                (100,),     # å•éšè—å±‚ 100 ä¸ªç¥ç»å…ƒ
                (50, 50),   # ä¸¤å±‚ï¼Œæ¯å±‚ 50 ä¸ª
                (100, 50),  # ä¸¤å±‚ï¼Œ100 å’Œ 50
            ]),
            'learning_rate': tune.grid_search([0.001, 0.01, 0.1]),
            'batch_size': tune.grid_search([32, 64, 128]),
        },
        metric='accuracy',
        mode='max',
        verbose=True
    )

    print("\nğŸ“Š è¯¦ç»†ç»“æœåˆ†æ:")
    print(f"  æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {analysis.best_result['accuracy']:.4f}")
    print(f"  æœ€ä½³è®­ç»ƒå‡†ç¡®ç‡: {analysis.best_result['train_accuracy']:.4f}")
    print(f"  è®­ç»ƒè€—æ—¶: {analysis.best_result['train_time']:.2f}s")
    print(f"  è¿­ä»£æ¬¡æ•°: {analysis.best_result['n_iter']}")
    print(f"  æœ€ä½³é…ç½®:")
    for key, value in analysis.best_config.items():
        print(f"    {key}: {value}")


# ============================================================
# Demo 4: å¯¹æ¯”ä¸åŒæœç´¢ç­–ç•¥
# ============================================================

def demo_4_compare_strategies():
    """
    æ¼”ç¤º 4: å¯¹æ¯”ç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢

    åœ¨åŒä¸€ä¸ªé—®é¢˜ä¸Šå¯¹æ¯”ä¸¤ç§æœç´¢ç­–ç•¥çš„æ•ˆæœ
    """
    print_section("Demo 4: å¯¹æ¯”ç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢")

    try:
        from sklearn.ensemble import GradientBoostingClassifier
        from sklearn.datasets import load_breast_cancer
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import accuracy_score
    except ImportError:
        print("âš ï¸  éœ€è¦å®‰è£… scikit-learn: pip install scikit-learn")
        return

    print("\nğŸ“Œ ç¬¬ 1 æ­¥: åŠ è½½æ•°æ®é›†ï¼ˆä¹³è…ºç™Œåˆ†ç±»ï¼‰")
    X, y = load_breast_cancer(return_X_y=True)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    print(f"  è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")

    def train_gbdt(config):
        """è®­ç»ƒæ¢¯åº¦æå‡æ ‘"""
        model = GradientBoostingClassifier(
            n_estimators=config['n_estimators'],
            learning_rate=config['learning_rate'],
            max_depth=config['max_depth'],
            random_state=42
        )

        model.fit(X_train, y_train)
        test_acc = accuracy_score(y_test, model.predict(X_test))

        return {'accuracy': test_acc}

    # ç­–ç•¥ 1: ç½‘æ ¼æœç´¢
    print("\nğŸ“Œ ç­–ç•¥ 1: ç½‘æ ¼æœç´¢")
    print("  æœç´¢ç©ºé—´: 3 Ã— 3 Ã— 3 = 27 ä¸ªé…ç½®")

    grid_analysis = tune.run(
        train_gbdt,
        config={
            'n_estimators': tune.grid_search([50, 100, 200]),
            'learning_rate': tune.grid_search([0.01, 0.1, 0.5]),
            'max_depth': tune.grid_search([3, 5, 7]),
        },
        metric='accuracy',
        mode='max',
        verbose=False  # ä¸æ‰“å°è¯¦ç»†ä¿¡æ¯
    )

    print(f"\n  âœ… ç½‘æ ¼æœç´¢ç»“æœ:")
    print(f"      è¯•éªŒæ•°: {len(grid_analysis.trials)}")
    print(f"      æœ€ä½³å‡†ç¡®ç‡: {grid_analysis.best_result['accuracy']:.4f}")
    print(f"      æœ€ä½³é…ç½®: {grid_analysis.best_config}")

    # ç­–ç•¥ 2: éšæœºæœç´¢
    print("\nğŸ“Œ ç­–ç•¥ 2: éšæœºæœç´¢ï¼ˆ10 æ¬¡é‡‡æ ·ï¼‰")

    random_analysis = tune.run(
        train_gbdt,
        config={
            'n_estimators': tune.random_search([50, 100, 200], num_samples=10),
            'learning_rate': tune.random_search([0.01, 0.1, 0.5], num_samples=10),
            'max_depth': tune.random_search([3, 5, 7], num_samples=10),
        },
        metric='accuracy',
        mode='max',
        search_alg='random',
        verbose=False
    )

    print(f"\n  âœ… éšæœºæœç´¢ç»“æœ:")
    print(f"      è¯•éªŒæ•°: {len(random_analysis.trials)}")
    print(f"      æœ€ä½³å‡†ç¡®ç‡: {random_analysis.best_result['accuracy']:.4f}")
    print(f"      æœ€ä½³é…ç½®: {random_analysis.best_config}")

    print("\nğŸ“Š å¯¹æ¯”æ€»ç»“:")
    print(f"  ç½‘æ ¼æœç´¢: {len(grid_analysis.trials)} æ¬¡è¯•éªŒ, "
          f"å‡†ç¡®ç‡ {grid_analysis.best_result['accuracy']:.4f}")
    print(f"  éšæœºæœç´¢: {len(random_analysis.trials)} æ¬¡è¯•éªŒ, "
          f"å‡†ç¡®ç‡ {random_analysis.best_result['accuracy']:.4f}")
    print(f"  ç»“è®º: éšæœºæœç´¢ç”¨æ›´å°‘çš„è¯•éªŒæ‰¾åˆ°äº† "
          f"{'æ›´å¥½' if random_analysis.best_result['accuracy'] > grid_analysis.best_result['accuracy'] else 'æ¥è¿‘'}"
          f"çš„ç»“æœ")


# ============================================================
# ä¸»å‡½æ•°
# ============================================================

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("  Mini-Ray è¶…å‚æ•°è°ƒä¼˜æ¼”ç¤º")
    print("=" * 70)
    print("\næœ¬æ¼”ç¤ºå±•ç¤º miniray.tune çš„å¼ºå¤§åŠŸèƒ½ï¼š")
    print("  â€¢ ç½‘æ ¼æœç´¢ï¼ˆGrid Searchï¼‰- ç©·ä¸¾æ‰€æœ‰ç»„åˆ")
    print("  â€¢ éšæœºæœç´¢ï¼ˆRandom Searchï¼‰- é«˜æ•ˆé‡‡æ ·")
    print("  â€¢ è‡ªåŠ¨å¹¶è¡Œæ‰§è¡Œ - åˆ©ç”¨ Mini-Ray åŠ é€Ÿ")
    print("  â€¢ ç»“æœåˆ†æ - è‡ªåŠ¨æ‰¾åˆ°æœ€ä½³é…ç½®")

    # åˆå§‹åŒ– Mini-Ray
    print("\n" + "=" * 70)
    print("  åˆå§‹åŒ– Mini-Ray")
    print("=" * 70)
    miniray.init(num_workers=4)
    time.sleep(0.5)

    try:
        # è¿è¡Œæ¼”ç¤º
        demo_1_simple_grid_search()
        demo_2_random_forest_tuning()
        demo_3_neural_network_tuning()
        demo_4_compare_strategies()

        # æ€»ç»“
        print_section("æ¼”ç¤ºå®Œæˆ")
        print("\nâœ… æ‰€æœ‰è¶…å‚æ•°è°ƒä¼˜æ¼”ç¤ºéƒ½æˆåŠŸå®Œæˆï¼")
        print("\nğŸ“Š æ¼”ç¤ºå†…å®¹å›é¡¾:")
        print("  1. ç®€å•ç½‘æ ¼æœç´¢ - æ‰¾åˆ°æœ€ä¼˜ (x, y) ç»„åˆ")
        print("  2. éšæœºæ£®æ—è°ƒä¼˜ - 36 ä¸ªé…ç½®ï¼Œæ‰¾åˆ°æœ€ä½³å‚æ•°")
        print("  3. ç¥ç»ç½‘ç»œè°ƒä¼˜ - ä¼˜åŒ–ç½‘ç»œç»“æ„å’Œå­¦ä¹ ç‡")
        print("  4. ç­–ç•¥å¯¹æ¯” - ç½‘æ ¼æœç´¢ vs éšæœºæœç´¢")

        print("\nğŸ’¡ å…³é”®è¦ç‚¹:")
        print("  â€¢ tune.run() æä¾›ç®€æ´çš„ API")
        print("  â€¢ tune.grid_search() ç”¨äºç½‘æ ¼æœç´¢")
        print("  â€¢ tune.random_search() ç”¨äºéšæœºæœç´¢")
        print("  â€¢ Analysis å¯¹è±¡æä¾›ç»“æœåˆ†æ")
        print("  â€¢ è‡ªåŠ¨åˆ©ç”¨ Mini-Ray å¹¶è¡ŒåŠ é€Ÿ")

        print("\nğŸš€ ä¸‹ä¸€æ­¥:")
        print("  â€¢ æŸ¥çœ‹ python/miniray/tune/ äº†è§£å®ç°ç»†èŠ‚")
        print("  â€¢ åœ¨è‡ªå·±çš„ ML é¡¹ç›®ä¸­ä½¿ç”¨ tune")
        print("  â€¢ å®ç°æ›´å¤šæœç´¢ç®—æ³•ï¼ˆè´å¶æ–¯ä¼˜åŒ–ç­‰ï¼‰")

    finally:
        # å…³é—­ Mini-Ray
        print("\n" + "=" * 70)
        print("  å…³é—­ Mini-Ray")
        print("=" * 70)
        miniray.shutdown()


if __name__ == "__main__":
    main()
